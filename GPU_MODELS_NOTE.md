# Примечание о GPU и моделях Ollama

## Как работает загрузка моделей в GPU

В Ollama модели **не загружаются в GPU заранее**. Они загружаются **по требованию** при первом запросе.

### Проверка загруженных моделей

```bash
# Проверить какие модели загружены в GPU на сервере 1
curl http://10.39.0.101:11434/api/ps

# Проверить какие модели загружены в GPU на сервере 2
curl http://10.39.0.6:11434/api/ps
```

**Результат:**
```json
{"models":[]}  // Нет загруженных моделей - это нормально!
```

### Что происходит при запросе

1. Пользователь отправляет запрос к модели
2. Ollama проверяет, загружена ли модель в GPU
3. Если нет - загружает модель в GPU память
4. Выполняет генерацию
5. Модель остается в GPU памяти (для быстрых последующих запросов)
6. Через некоторое время без использования - выгружается

### Как загрузить модель в GPU заранее

Если нужно "прогреть" модель (загрузить в GPU до использования):

```bash
# Загрузить модель (просто запросить что-то простое)
curl http://10.39.0.101:11434/api/generate -d '{
  "model": "huihui_ai/deepseek-r1-abliterated:8b",
  "prompt": "Hi",
  "stream": false
}'

# Проверить что модель загружена
curl http://10.39.0.101:11434/api/ps
```

Теперь модель будет в GPU памяти и готова к использованию.

### В контексте AARD

- **Это нормальное поведение** - не требует предварительной загрузки
- Модели загрузятся автоматически при первом запросе
- Health check проверяет **доступность сервера**, а не наличие модели в GPU
- Для продакшена можно добавить "прогрев" моделей при старте приложения

### Рекомендации

1. **Для разработки:** Загрузка по требованию - это нормально
2. **Для продакшена:** Рассмотреть "прогрев" популярных моделей при старте
3. **Мониторинг:** Отслеживать использование GPU через `/api/ps`

