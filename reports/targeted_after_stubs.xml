<?xml version="1.0" encoding="utf-8"?><testsuites><testsuite name="pytest" errors="2" failures="9" skipped="0" tests="16" time="30.409" timestamp="2025-12-22T11:28:03.784990" hostname="zloiws"><testcase classname="tests.cli.test_migrations_cli" name="test_build_parser_and_call_migrate" time="0.004"><failure message="SystemExit: 0">monkeypatch = &lt;_pytest.monkeypatch.MonkeyPatch object at 0x000001A25CA25FA0&gt;

    def test_build_parser_and_call_migrate(monkeypatch):
        m = importlib.import_module("backend.cli.migrations")
        parser = m.build_parser()
        # just ensure parser builds and help can be printed
        parser.format_help()
        # call help path
&gt;       rc = m.main(["--help"])

backend\tests\cli\test_migrations_cli.py:10: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
backend\cli\migrations.py:75: in main
    args = p.parse_args(argv)
C:\Program Files\Python312\Lib\argparse.py:1895: in parse_args
    args, argv = self.parse_known_args(args, namespace)
C:\Program Files\Python312\Lib\argparse.py:1931: in parse_known_args
    namespace, args = self._parse_known_args(args, namespace)
C:\Program Files\Python312\Lib\argparse.py:2168: in _parse_known_args
    start_index = consume_optional(start_index)
C:\Program Files\Python312\Lib\argparse.py:2099: in consume_optional
    take_action(action, args, option_string)
C:\Program Files\Python312\Lib\argparse.py:2006: in take_action
    action(self, namespace, argument_values, option_string)
C:\Program Files\Python312\Lib\argparse.py:1144: in __call__
    parser.exit()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = ArgumentParser(prog='migrations', usage=None, description=None, formatter_class=&lt;class 'argparse.HelpFormatter'&gt;, conflict_handler='error', add_help=True)
status = 0, message = None

    def exit(self, status=0, message=None):
        if message:
            self._print_message(message, _sys.stderr)
&gt;       _sys.exit(status)
E       SystemExit: 0

C:\Program Files\Python312\Lib\argparse.py:2659: SystemExit</failure></testcase><testcase classname="tests.docs.test_service_docs_present" name="test_service_docs_exist" time="0.001"><failure message="AssertionError: Missing service docs: ['ApplicationCore.md', 'HTTPAPILayer.md', 'DataORMLayer.md', 'BusinessServices.md', 'AgentsAndPlanning.md', 'DecisionComponents.md', 'ToolsExternalIntegrations.md', 'RegistryServiceDiscovery.md', 'SecurityAuthPermissions.md', 'MemoryConversationStorage.md', 'OpsMigrations.md', 'UtilitiesObservability.md']&#10;assert not ['ApplicationCore.md', 'HTTPAPILayer.md', 'DataORMLayer.md', 'BusinessServices.md', 'AgentsAndPlanning.md', 'DecisionComponents.md', ...]">def test_service_docs_exist():
        base = os.path.join(os.path.dirname(__file__), "..", "docs", "services")
        base = os.path.normpath(base)
        missing = []
        for fn in EXPECTED:
            path = os.path.join(base, fn)
            if not os.path.exists(path):
                missing.append(fn)
&gt;       assert not missing, f"Missing service docs: {missing}"
E       AssertionError: Missing service docs: ['ApplicationCore.md', 'HTTPAPILayer.md', 'DataORMLayer.md', 'BusinessServices.md', 'AgentsAndPlanning.md', 'DecisionComponents.md', 'ToolsExternalIntegrations.md', 'RegistryServiceDiscovery.md', 'SecurityAuthPermissions.md', 'MemoryConversationStorage.md', 'OpsMigrations.md', 'UtilitiesObservability.md']
E       assert not ['ApplicationCore.md', 'HTTPAPILayer.md', 'DataORMLayer.md', 'BusinessServices.md', 'AgentsAndPlanning.md', 'DecisionComponents.md', ...]

backend\tests\docs\test_service_docs_present.py:31: AssertionError</failure></testcase><testcase classname="tests.integration.test_planning_api_simple" name="test_health" time="4.074" /><testcase classname="tests.integration.test_planning_api_simple" name="test_list_plans" time="4.046" /><testcase classname="tests.integration.test_planning_api_simple" name="test_get_plan" time="4.047" /><testcase classname="tests.integration.test_planning_api_simple" name="test_get_plan_status" time="4.074" /><testcase classname="tests.integration.test_planning_api_simple" name="test_filter_plans" time="12.284" /><testcase classname="tests.quick_test.TestWorkflowEngineBasic" name="test_initialization" time="0.001"><failure message="AssertionError: assert 'INITIALIZED' == &lt;WorkflowStat...'initialized'&gt;&#10;  - initialized&#10;  + INITIALIZED">self = &lt;tests.integration.test_workflow_engine.TestWorkflowEngineBasic object at 0x000001A25CA252E0&gt;
workflow_engine = &lt;Mock id='1796850769920'&gt;
execution_context = &lt;tests.conftest.execution_context.&lt;locals&gt;.ExecutionContextStub object at 0x000001A25CA6E2A0&gt;

    def test_initialization(self, workflow_engine, execution_context):
        """Тест инициализации workflow"""
        workflow_engine.initialize(
            user_request="Test request",
            username="test_user",
            interaction_type="test"
        )
    
&gt;       assert workflow_engine.get_current_state() == WorkflowState.INITIALIZED
E       AssertionError: assert 'INITIALIZED' == &lt;WorkflowStat...'initialized'&gt;
E         - initialized
E         + INITIALIZED

backend\tests\integration\test_workflow_engine.py:56: AssertionError</failure></testcase><testcase classname="tests.quick_test.TestWorkflowEngineBasic" name="test_transition_to_parsing" time="0.001"><failure message="AssertionError: assert &lt;Mock name='mock.transition_to()' id='1796850771504'&gt; is True">self = &lt;tests.integration.test_workflow_engine.TestWorkflowEngineBasic object at 0x000001A25CA25460&gt;
workflow_engine = &lt;Mock id='1796850770688'&gt;

    def test_transition_to_parsing(self, workflow_engine):
        """Тест перехода в состояние PARSING"""
        workflow_engine.initialize("Test", "test_user")
    
        success = workflow_engine.transition_to(
            WorkflowState.PARSING,
            "Parsing request"
        )
    
&gt;       assert success is True
E       AssertionError: assert &lt;Mock name='mock.transition_to()' id='1796850771504'&gt; is True

backend\tests\integration\test_workflow_engine.py:68: AssertionError</failure></testcase><testcase classname="tests.quick_test.TestWorkflowEngineBasic" name="test_transition_to_planning" time="0.001"><failure message="AssertionError: assert &lt;Mock name='mock.transition_to()' id='1796851614880'&gt; is True">self = &lt;tests.integration.test_workflow_engine.TestWorkflowEngineBasic object at 0x000001A25CA255B0&gt;
workflow_engine = &lt;Mock id='1796851628992'&gt;

    def test_transition_to_planning(self, workflow_engine):
        """Тест перехода в состояние PLANNING"""
        workflow_engine.initialize("Test", "test_user")
        workflow_engine.transition_to(WorkflowState.PARSING, "Parsing")
    
        success = workflow_engine.transition_to(
            WorkflowState.PLANNING,
            "Starting planning"
        )
    
&gt;       assert success is True
E       AssertionError: assert &lt;Mock name='mock.transition_to()' id='1796851614880'&gt; is True

backend\tests\integration\test_workflow_engine.py:81: AssertionError</failure></testcase><testcase classname="tests.quick_test.TestWorkflowEngineBasic" name="test_invalid_transition" time="0.001"><failure message="AssertionError: assert &lt;Mock name='mock.transition_to()' id='1796851618240'&gt; is False">self = &lt;tests.integration.test_workflow_engine.TestWorkflowEngineBasic object at 0x000001A25CA25700&gt;
workflow_engine = &lt;Mock id='1796851617232'&gt;

    def test_invalid_transition(self, workflow_engine):
        """Тест недопустимого перехода"""
        workflow_engine.initialize("Test", "test_user")
    
        # Попытка перейти из INITIALIZED напрямую в EXECUTING (недопустимо)
        success = workflow_engine.transition_to(
            WorkflowState.EXECUTING,
            "Invalid transition"
        )
    
&gt;       assert success is False
E       AssertionError: assert &lt;Mock name='mock.transition_to()' id='1796851618240'&gt; is False

backend\tests\integration\test_workflow_engine.py:94: AssertionError</failure></testcase><testcase classname="tests.quick_test.TestWorkflowEngineBasic" name="test_forced_transition" time="0.001"><failure message="AssertionError: assert &lt;Mock name='mock.transition_to()' id='1796851622656'&gt; is True">self = &lt;tests.integration.test_workflow_engine.TestWorkflowEngineBasic object at 0x000001A25CA25850&gt;
workflow_engine = &lt;Mock id='1796851621552'&gt;

    def test_forced_transition(self, workflow_engine):
        """Тест принудительного перехода"""
        workflow_engine.initialize("Test", "test_user")
    
        # Принудительный переход (например, для ошибок)
        success = workflow_engine.transition_to(
            WorkflowState.FAILED,
            "Forced failure",
            force=True
        )
    
&gt;       assert success is True
E       AssertionError: assert &lt;Mock name='mock.transition_to()' id='1796851622656'&gt; is True

backend\tests\integration\test_workflow_engine.py:108: AssertionError</failure></testcase><testcase classname="tests.quick_test.TestWorkflowEngineBasic" name="test_transition_history" time="0.001"><failure message="assert 0 == 2&#10; +  where 0 = len([])">self = &lt;tests.integration.test_workflow_engine.TestWorkflowEngineBasic object at 0x000001A25CA259A0&gt;
workflow_engine = &lt;Mock id='1796850481488'&gt;

    def test_transition_history(self, workflow_engine):
        """Тест истории переходов"""
        workflow_engine.initialize("Test", "test_user")
        workflow_engine.transition_to(WorkflowState.PARSING, "Step 1")
        workflow_engine.transition_to(WorkflowState.PLANNING, "Step 2")
    
        history = workflow_engine.get_transition_history()
    
&gt;       assert len(history) == 2
E       assert 0 == 2
E        +  where 0 = len([])

backend\tests\integration\test_workflow_engine.py:119: AssertionError</failure></testcase><testcase classname="tests.quick_test" name="test_level1_basic_context_creation" time="0.003"><error message="failed on setup with &quot;file C:\work\AARD\backend\tests\integration\test_phase3_full_integration.py, line 103&#10;  @pytest.mark.asyncio&#10;  async def test_level1_basic_context_creation(db_session):&#10;      &quot;&quot;&quot;Уровень 1: Базовое создание ExecutionContext&quot;&quot;&quot;&#10;      print(&quot;\n=== УРОВЕНЬ 1: Базовое создание ExecutionContext ===&quot;)&#10;&#10;      # Создание контекста из сессии&#10;      context = ExecutionContext.from_db_session(db_session)&#10;&#10;      assert context is not None&#10;      assert context.db == db_session&#10;      assert context.workflow_id is not None&#10;      assert len(context.workflow_id) &gt; 0&#10;      # trace_id может быть None, если нет активного OpenTelemetry span (нормально для тестов)&#10;      # assert context.trace_id is not None&#10;&#10;      print(f&quot;✅ ExecutionContext создан: workflow_id={context.workflow_id[:8]}...&quot;)&#10;E       fixture 'db_session' not found&#10;&gt;       available fixtures: anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, client, db, doctest_namespace, ensure_service_registry_initialized, event_loop, execution_context, monkeypatch, plan_id, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, workflow_engine&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;C:\work\AARD\backend\tests\integration\test_phase3_full_integration.py:103&quot;">file C:\work\AARD\backend\tests\integration\test_phase3_full_integration.py, line 103
  @pytest.mark.asyncio
  async def test_level1_basic_context_creation(db_session):
      """Уровень 1: Базовое создание ExecutionContext"""
      print("\n=== УРОВЕНЬ 1: Базовое создание ExecutionContext ===")

      # Создание контекста из сессии
      context = ExecutionContext.from_db_session(db_session)

      assert context is not None
      assert context.db == db_session
      assert context.workflow_id is not None
      assert len(context.workflow_id) &gt; 0
      # trace_id может быть None, если нет активного OpenTelemetry span (нормально для тестов)
      # assert context.trace_id is not None

      print(f"✅ ExecutionContext создан: workflow_id={context.workflow_id[:8]}...")
E       fixture 'db_session' not found
&gt;       available fixtures: anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, client, db, doctest_namespace, ensure_service_registry_initialized, event_loop, execution_context, monkeypatch, plan_id, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, workflow_engine
&gt;       use 'pytest --fixtures [testpath]' for help on them.

C:\work\AARD\backend\tests\integration\test_phase3_full_integration.py:103</error></testcase><testcase classname="tests.quick_test.TestWorkflowEngineIntegration" name="test_workflow_engine_initialization_in_orchestrator" time="0.003"><failure message="AttributeError: 'ExecutionContextStub' object has no attribute 'user_id'">self = &lt;tests.integration.test_phase4_integration.TestWorkflowEngineIntegration object at 0x000001A25CA25B50&gt;
execution_context = &lt;tests.conftest.execution_context.&lt;locals&gt;.ExecutionContextStub object at 0x000001A25CA6FBF0&gt;

    @pytest.mark.asyncio
    async def test_workflow_engine_initialization_in_orchestrator(self, execution_context):
        """Тест инициализации WorkflowEngine в RequestOrchestrator"""
        orchestrator = RequestOrchestrator()
    
        # Создаем минимальный запрос для инициализации workflow
        message = "Test message"
    
        # Мокаем LLM вызовы чтобы не делать реальные запросы
        with patch('app.core.request_orchestrator.OllamaClient') as mock_ollama:
            mock_client = Mock()
            mock_client.generate = AsyncMock(return_value=Mock(
                response="Test response",
                model="test-model",
                tokens_used=100
            ))
            mock_ollama.return_value = mock_client
    
&gt;           result = await orchestrator.process_request(
                message=message,
                context=execution_context,
                task_type="general_chat"
            )

backend\tests\integration\test_phase4_integration.py:125: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = &lt;app.core.request_orchestrator.RequestOrchestrator object at 0x000001A25CA6FD10&gt;
message = 'Test message'
context = &lt;tests.conftest.execution_context.&lt;locals&gt;.ExecutionContextStub object at 0x000001A25CA6FBF0&gt;
task_type = 'general_chat', model = None, server_id = None, temperature = 0.7

    async def process_request(
        self,
        message: str,
        context: ExecutionContext,
        task_type: Optional[str] = None,
        model: Optional[str] = None,
        server_id: Optional[str] = None,
        temperature: float = 0.7
    ) -&gt; OrchestrationResult:
        """
        Обработать запрос пользователя
    
        Args:
            message: Сообщение пользователя
            context: ExecutionContext
            task_type: Опциональный тип задачи
            model: Опциональная модель
            server_id: Опциональный ID сервера
            temperature: Temperature для генерации
    
        Returns:
            OrchestrationResult с результатом обработки
        """
        start_time = time.time()
    
        # Создать WorkflowEngine для управления состояниями
        workflow_engine = WorkflowEngine.from_context(context)
        workflow_engine.initialize(
            user_request=message,
&gt;           username=context.user_id or "user",
            interaction_type=task_type or "chat"
        )
E       AttributeError: 'ExecutionContextStub' object has no attribute 'user_id'

backend\app\core\request_orchestrator.py:90: AttributeError</failure></testcase><testcase classname="tests.quick_test.TestWorkflowEngineIntegration" name="test_workflow_state_transitions_in_code_generation" time="0.002"><error message="failed on setup with &quot;file C:\work\AARD\backend\tests\integration\test_phase4_integration.py, line 139&#10;      @pytest.mark.asyncio&#10;      async def test_workflow_state_transitions_in_code_generation(&#10;          self, execution_context, real_model_and_server, test_agent&#10;      ):&#10;          &quot;&quot;&quot;Тест переходов состояний workflow при генерации кода&quot;&quot;&quot;&#10;          model, server = real_model_and_server&#10;&#10;          # Сохраняем модель и server в контекст&#10;          execution_context.metadata = {&#10;              &quot;model&quot;: model.model_name,&#10;              &quot;server_id&quot;: str(server.id),&#10;              &quot;server_url&quot;: server.get_api_url()&#10;          }&#10;&#10;          orchestrator = RequestOrchestrator()&#10;&#10;          # Простой запрос на генерацию кода&#10;          message = &quot;Create a function that adds two numbers&quot;&#10;&#10;          try:&#10;              result = await orchestrator.process_request(&#10;                  message=message,&#10;                  context=execution_context,&#10;                  task_type=&quot;code_generation&quot;,&#10;                  model=model.model_name,&#10;                  server_id=str(server.id)&#10;              )&#10;&#10;              # Проверяем, что workflow прошел через нужные состояния&#10;              workflow_engine = WorkflowEngine.from_context(execution_context)&#10;              current_state = workflow_engine.get_current_state()&#10;&#10;              # Workflow должен быть в финальном состоянии (COMPLETED или FAILED)&#10;              assert current_state in [&#10;                  WorkflowState.COMPLETED,&#10;                  WorkflowState.FAILED,&#10;                  WorkflowState.CANCELLED&#10;              ], f&quot;Unexpected workflow state: {current_state}&quot;&#10;&#10;              # Проверяем историю переходов&#10;              history = workflow_engine.get_transition_history()&#10;              assert len(history) &gt; 0, &quot;Workflow should have transition history&quot;&#10;&#10;              # Первое состояние должно быть INITIALIZED&#10;              assert history[0].from_state is None or history[0].from_state == WorkflowState.INITIALIZED&#10;&#10;          except Exception as e:&#10;              pytest.skip(f&quot;Test requires working LLM: {e}&quot;)&#10;E       fixture 'real_model_and_server' not found&#10;&gt;       available fixtures: anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, client, db, doctest_namespace, ensure_service_registry_initialized, event_loop, execution_context, monkeypatch, plan_id, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, workflow_engine&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;C:\work\AARD\backend\tests\integration\test_phase4_integration.py:139&quot;">file C:\work\AARD\backend\tests\integration\test_phase4_integration.py, line 139
      @pytest.mark.asyncio
      async def test_workflow_state_transitions_in_code_generation(
          self, execution_context, real_model_and_server, test_agent
      ):
          """Тест переходов состояний workflow при генерации кода"""
          model, server = real_model_and_server

          # Сохраняем модель и server в контекст
          execution_context.metadata = {
              "model": model.model_name,
              "server_id": str(server.id),
              "server_url": server.get_api_url()
          }

          orchestrator = RequestOrchestrator()

          # Простой запрос на генерацию кода
          message = "Create a function that adds two numbers"

          try:
              result = await orchestrator.process_request(
                  message=message,
                  context=execution_context,
                  task_type="code_generation",
                  model=model.model_name,
                  server_id=str(server.id)
              )

              # Проверяем, что workflow прошел через нужные состояния
              workflow_engine = WorkflowEngine.from_context(execution_context)
              current_state = workflow_engine.get_current_state()

              # Workflow должен быть в финальном состоянии (COMPLETED или FAILED)
              assert current_state in [
                  WorkflowState.COMPLETED,
                  WorkflowState.FAILED,
                  WorkflowState.CANCELLED
              ], f"Unexpected workflow state: {current_state}"

              # Проверяем историю переходов
              history = workflow_engine.get_transition_history()
              assert len(history) &gt; 0, "Workflow should have transition history"

              # Первое состояние должно быть INITIALIZED
              assert history[0].from_state is None or history[0].from_state == WorkflowState.INITIALIZED

          except Exception as e:
              pytest.skip(f"Test requires working LLM: {e}")
E       fixture 'real_model_and_server' not found
&gt;       available fixtures: anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, client, db, doctest_namespace, ensure_service_registry_initialized, event_loop, execution_context, monkeypatch, plan_id, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, workflow_engine
&gt;       use 'pytest --fixtures [testpath]' for help on them.

C:\work\AARD\backend\tests\integration\test_phase4_integration.py:139</error></testcase></testsuite></testsuites>