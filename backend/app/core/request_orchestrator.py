"""
Request Orchestrator - —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤
"""
import time
from typing import Any, Dict, Optional, Tuple
from uuid import uuid4

from app.components.interpretation_service import InterpretationService
from app.components.semantic_validator import SemanticValidator
from app.core.execution_context import ExecutionContext
from app.core.logging_config import LoggingConfig
from app.core.model_selector import ModelSelector
from app.core.ollama_client import OllamaClient, TaskType
from app.core.prompt_manager import PromptManager
from app.core.request_router import RequestType, determine_request_type
from app.core.service_registry import get_service_registry
from app.core.workflow_engine import WorkflowEngine, WorkflowState
from app.models.interpretation import DecisionTimeline
from app.models.task import Task, TaskStatus
from app.services.ollama_service import OllamaService
from app.services.planning_hypothesis_service import PlanningHypothesisService
from sqlalchemy.orm import Session

logger = LoggingConfig.get_logger(__name__)


class OrchestrationResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–∞"""
    
    def __init__(
        self,
        response: str,
        model: str = "unknown",
        task_type: str = "unknown",
        duration_ms: Optional[int] = None,
        metadata: Optional[Dict[str, Any]] = None
    ):
        self.response = response
        self.model = model
        self.task_type = task_type
        self.duration_ms = duration_ms
        self.metadata = metadata or {}


class RequestOrchestrator:
    """
    –¶–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
    
    –£–ø—Ä–∞–≤–ª—è–µ—Ç –≤—Å–µ–º workflow –æ—Ç –∑–∞–ø—Ä–æ—Å–∞ –¥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞:
    - –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞
    - –í—ã–±–∏—Ä–∞–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ–±—Ä–∞–±–æ—Ç–∫–∏
    - –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Å–µ—Ä–≤–∏—Å—ã
    - –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ—à–∏–±–∫–∏ –∏ fallback
    """
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä–∞"""
        self.registry = get_service_registry()
        # WorkflowEngine –±—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ —Å –µ–≥–æ ExecutionContext
    
    async def process_request(
        self,
        message: str,
        context: ExecutionContext,
        task_type: Optional[str] = None,
        model: Optional[str] = None,
        server_id: Optional[str] = None,
        temperature: float = 0.7
    ) -> OrchestrationResult:
        """
        –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        
        Args:
            message: –°–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            context: ExecutionContext
            task_type: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ç–∏–ø –∑–∞–¥–∞—á–∏
            model: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å
            server_id: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π ID —Å–µ—Ä–≤–µ—Ä–∞
            temperature: Temperature –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            
        Returns:
            OrchestrationResult —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        start_time = time.time()
        
        # –°–æ–∑–¥–∞—Ç—å WorkflowEngine –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è–º–∏
        workflow_engine = WorkflowEngine.from_context(context)
        workflow_engine.initialize(
            user_request=message,
            username=context.user_id or "user",
            interaction_type=task_type or "chat"
        )
        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å workflow_engine –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∏–∑ –¥—Ä—É–≥–∏—Ö –º–µ—Å—Ç
        context.set_workflow_engine(workflow_engine)
        
        # –°–æ–∑–¥–∞—Ç—å PromptManager –∏ –¥–æ–±–∞–≤–∏—Ç—å –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç
        prompt_manager = PromptManager(context)
        context.set_prompt_manager(prompt_manager)
        
        # Interpretation step: run explicit interpretation layer before routing/planning
        try:
            interpretation_service = InterpretationService(context.db)
            structured_intent = await interpretation_service.interpret(message, context)
            semantic_validator = SemanticValidator()
            validation = await semantic_validator.validate_intent(structured_intent)
            # Ensure metadata exists
            if not getattr(context, "metadata", None):
                context.metadata = context.metadata or {}
            # Persist both typed contract and legacy payload (for compatibility with existing services)
            context.metadata["structured_intent"] = structured_intent.model_dump()
            context.metadata["interpretation"] = structured_intent.metadata.get("legacy") or structured_intent.model_dump()

            # If interpretation requires clarification, ask user immediately
            if validation.status == "clarification_required":
                questions = validation.clarification_questions or structured_intent.clarification_questions or []
                clarification_text = "–¢—Ä–µ–±—É–µ—Ç—Å—è —É—Ç–æ—á–Ω–µ–Ω–∏–µ:\n" + ("\n".join(f"- {q}" for q in questions) if questions else "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É—Ç–æ—á–Ω–∏—Ç–µ –∑–∞–ø—Ä–æ—Å.")
                return OrchestrationResult(
                    response=clarification_text,
                    model="none",
                    task_type="clarification",
                    metadata={"clarification_required": True, "questions": questions, "validation": validation.model_dump()}
                )

            # Planning step: generate plan hypotheses for complex requests
            try:
                # Find the timeline created by interpretation service
                session_id = str(context.workflow_id) if hasattr(context, "workflow_id") and context.workflow_id else str(uuid4())
                timeline = context.db.query(DecisionTimeline).filter(DecisionTimeline.session_id == session_id).first()

                if timeline:
                    planning_service = PlanningHypothesisService(context.db)
                    hypotheses = await planning_service.generate_plan_hypotheses(
                        timeline.id,
                        context.metadata.get("interpretation") or {}
                    )

                    # Store hypotheses in context for later use
                    context.metadata["plan_hypotheses"] = [
                        {
                            "id": str(h.id),
                            "name": h.name,
                            "confidence": h.confidence,
                            "lifecycle": h.lifecycle.value
                        }
                        for h in hypotheses
                    ]

                    logger.info(f"Generated {len(hypotheses)} plan hypotheses for timeline {timeline.id}")

            except Exception as e:
                # Non-fatal: log and continue (fallback to existing flow)
                logger.warning(f"Planning hypothesis generation failed: {e}", exc_info=True)

        except Exception as e:
            # Non-fatal: log and continue (fallback to existing flow)
            logger.warning(f"Interpretation step failed: {e}", exc_info=True)
        
        # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞
        request_type, request_metadata = determine_request_type(message, task_type)
        
        # –ü–µ—Ä–µ—Ö–æ–¥ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–µ PARSING
        workflow_engine.transition_to(
            WorkflowState.PARSING,
            f"–û–ø—Ä–µ–¥–µ–ª–µ–Ω —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞: {request_type.value}",
            metadata={"request_type": request_type.value, "metadata": request_metadata}
        )
        
        logger.info(
            f"Processing request: {request_type.value}",
            extra={
                "request_type": request_type.value,
                "workflow_id": context.workflow_id,
                "message_preview": message[:100]
            }
        )
        
        # –°–æ–±—ã—Ç–∏–µ —É–∂–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ —á–µ—Ä–µ–∑ WorkflowEngine.transition_to()
        
        try:
            # –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –∑–∞–ø—Ä–æ—Å–∞
            if request_type == RequestType.SIMPLE_QUESTION:
                result = await self._handle_simple_question(
                    message, context, task_type, model, server_id, temperature
                )
            elif request_type == RequestType.INFORMATION_QUERY:
                result = await self._handle_information_query(
                    message, context, request_metadata
                )
            elif request_type == RequestType.CODE_GENERATION:
                result = await self._handle_code_generation(
                    message, context, request_metadata, model, server_id
                )
            elif request_type == RequestType.COMPLEX_TASK:
                result = await self._handle_complex_task(
                    message, context, request_metadata
                )
            elif request_type == RequestType.PLANNING_ONLY:
                result = await self._handle_planning_only(
                    message, context, request_metadata, model, server_id
                )
            else:
                # Fallback –∫ –ø—Ä–æ—Å—Ç–æ–º—É –≤–æ–ø—Ä–æ—Å—É
                result = await self._handle_simple_question(
                    message, context, task_type, model, server_id, temperature
                )
            
            duration_ms = int((time.time() - start_time) * 1000)
            result.duration_ms = duration_ms
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏ —É–ª—É—á—à–∏—Ç—å –ø—Ä–æ–º–ø—Ç—ã –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏
            try:
                improvement_results = await prompt_manager.analyze_and_improve_prompts()
                if improvement_results.get("improved", 0) > 0:
                    logger.info(
                        f"Created {improvement_results['improved']} improved prompt versions",
                        extra={"workflow_id": context.workflow_id, "results": improvement_results}
                    )
            except Exception as e:
                logger.warning(f"Failed to analyze and improve prompts: {e}", exc_info=True)
            
            # –û—Ç–º–µ—Ç–∏—Ç—å workflow –∫–∞–∫ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–π
            try:
                workflow_engine = context.workflow_engine or WorkflowEngine.from_context(context)
                if workflow_engine and workflow_engine.get_current_state() not in [WorkflowState.COMPLETED, WorkflowState.FAILED, WorkflowState.CANCELLED]:
                    workflow_engine.mark_completed(result=result.response[:200] if result.response else None)
            except Exception:
                pass  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è
            
            # –°–æ–±—ã—Ç–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —É–∂–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ —á–µ—Ä–µ–∑ WorkflowEngine.mark_completed()
            
            return result
            
        except Exception as e:
            logger.error(
                f"Error processing request: {e}",
                exc_info=True,
                extra={"workflow_id": context.workflow_id}
            )
            
            # –û—Ç–º–µ—Ç–∏—Ç—å workflow –∫–∞–∫ –ø—Ä–æ–≤–∞–ª–µ–Ω–Ω—ã–π
            try:
                workflow_engine = getattr(context, 'workflow_engine', None)
                if workflow_engine:
                    workflow_engine.mark_failed(
                        error=str(e),
                        error_details={"request_type": request_type.value if 'request_type' in locals() else "unknown", "stage": "processing"}
                    )
            except Exception:
                pass  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è
            
            # –ü–æ–ø—ã—Ç–∫–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ replanning –¥–ª—è CODE_GENERATION –∏ COMPLEX_TASK
            workflow_engine = getattr(context, 'workflow_engine', None)
            if request_type in [RequestType.CODE_GENERATION, RequestType.COMPLEX_TASK] and workflow_engine:
                try:
                    logger.info("Attempting automatic replanning after error")
                    # –ü–µ—Ä–µ—Ö–æ–¥ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–µ RETRYING
                    workflow_engine.transition_to(
                        WorkflowState.RETRYING,
                        "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π replanning –ø–æ—Å–ª–µ –æ—à–∏–±–∫–∏",
                        metadata={"original_error": str(e)}
                    )
                    
                    # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π replanning - –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç–æ–π –ø–æ–¥—Ö–æ–¥
                    result = await self._handle_simple_question(
                        message, context, task_type, model, server_id, temperature
                    )
                    duration_ms = int((time.time() - start_time) * 1000)
                    result.duration_ms = duration_ms
                    
                    # –ï—Å–ª–∏ replanning —É—Å–ø–µ—à–µ–Ω, –æ—Ç–º–µ—Ç–∏—Ç—å –∫–∞–∫ completed
                    workflow_engine.mark_completed(result=result.response[:200])
                    
                    return result
                except Exception as replanning_error:
                    logger.warning(f"Replanning failed: {replanning_error}")
            
            # Fallback –∫ –ø—Ä–æ—Å—Ç–æ–º—É –≤–æ–ø—Ä–æ—Å—É –ø—Ä–∏ –æ—à–∏–±–∫–µ
            try:
                if workflow_engine:
                    workflow_engine.transition_to(
                        WorkflowState.RETRYING,
                        "Fallback –∫ –ø—Ä–æ—Å—Ç–æ–º—É –≤–æ–ø—Ä–æ—Å—É",
                        metadata={"original_error": str(e)}
                    )
                
                result = await self._handle_simple_question(
                    message, context, task_type, model, server_id, temperature
                )
                duration_ms = int((time.time() - start_time) * 1000)
                result.duration_ms = duration_ms
                
                # –ï—Å–ª–∏ fallback —É—Å–ø–µ—à–µ–Ω, –æ—Ç–º–µ—Ç–∏—Ç—å –∫–∞–∫ completed
                if workflow_engine:
                    workflow_engine.mark_completed(result=result.response[:200])
                
                return result
            except Exception as fallback_error:
                logger.error(
                    f"Fallback also failed: {fallback_error}",
                    exc_info=True
                )
                
                # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—à–∏–±–∫–∞
                if workflow_engine:
                    workflow_engine.mark_failed(
                        error=f"All fallback strategies failed: {str(fallback_error)}",
                        error_details={"original_error": str(e), "fallback_error": str(fallback_error)}
                    )
                
                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—à–∏–±–∫—É
                return OrchestrationResult(
                    response=f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞: {str(e)}",
                    model="error",
                    task_type=request_type.value,
                    duration_ms=int((time.time() - start_time) * 1000),
                    metadata={"error": str(e), "fallback_error": str(fallback_error)}
                )
    
    async def _handle_simple_question(
        self,
        message: str,
        context: ExecutionContext,
        task_type: Optional[str] = None,
        model: Optional[str] = None,
        server_id: Optional[str] = None,
        temperature: float = 0.7
    ) -> OrchestrationResult:
        """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –ø—Ä–æ—Å—Ç–æ–π –≤–æ–ø—Ä–æ—Å - –ø—Ä—è–º–æ–π –æ—Ç–≤–µ—Ç –æ—Ç LLM"""
        logger.debug("Handling simple question with direct LLM")
        
        prompt_start_time = time.time()
        prompt_id = None
        
        # –ü–æ–ª—É—á–∏—Ç—å system prompt —á–µ—Ä–µ–∑ PromptManager
        system_prompt = await self._get_system_prompt(context)
        
        # –ó–∞–ø–∏—Å–∞—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞, –µ—Å–ª–∏ –µ—Å—Ç—å PromptManager
        if context.prompt_manager and system_prompt:
            try:
                prompt = await context.prompt_manager.get_prompt_for_stage("planning")
                if prompt:
                    prompt_id = prompt.id
            except Exception as e:
                logger.debug(f"Could not get prompt for recording: {e}")
        
        # –í—ã–±—Ä–∞—Ç—å –º–æ–¥–µ–ª—å –∏ —Å–µ—Ä–≤–µ—Ä
        selected_model, selected_server = self._select_model_and_server(
            context.db, task_type, model, server_id
        )
        
        if not selected_model or not selected_server:
            raise ValueError("No available model or server found")
        
        # –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç
        execution_start = time.time()
        try:
            ollama_client = OllamaClient()
            response = await ollama_client.generate(
                prompt=message,
                task_type=TaskType.DEFAULT if not task_type else TaskType(task_type),
                model=selected_model,
                server_url=selected_server.get_api_url(),
                system_prompt=system_prompt,
                temperature=temperature
            )
            
            execution_time_ms = (time.time() - execution_start) * 1000
            
            # –ó–∞–ø–∏—Å–∞—Ç—å —É—Å–ø–µ—à–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞
            if context.prompt_manager and prompt_id:
                await context.prompt_manager.record_prompt_usage(
                    prompt_id=prompt_id,
                    success=True,
                    execution_time_ms=execution_time_ms,
                    stage="simple_question"
                )
            
            return OrchestrationResult(
                response=response.response,
                model=selected_model,
                task_type=task_type or "general_chat",
                metadata={"server_id": str(selected_server.id)}
            )
        except Exception as e:
            execution_time_ms = (time.time() - execution_start) * 1000
            
            # –ó–∞–ø–∏—Å–∞—Ç—å –Ω–µ—É–¥–∞—á–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞
            if context.prompt_manager and prompt_id:
                await context.prompt_manager.record_prompt_usage(
                    prompt_id=prompt_id,
                    success=False,
                    execution_time_ms=execution_time_ms,
                    stage="simple_question"
                )
            
            raise
    
    async def _handle_information_query(
        self,
        message: str,
        context: ExecutionContext,
        metadata: Dict[str, Any]
    ) -> OrchestrationResult:
        """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å - –Ω—É–∂–µ–Ω –ø–æ–∏—Å–∫"""
        logger.debug("Handling information query")
        
        # –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è MemoryService –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ –ø–∞–º—è—Ç–∏
        from app.services.memory_service import MemoryService
        
        memory_service = MemoryService(context)
        
        # –ü–æ–ª—É—á–∏—Ç—å WorkflowEngine –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        workflow_engine = getattr(context, 'workflow_engine', None)
        if workflow_engine:
            workflow_engine.transition_to(
                WorkflowState.PARSING,
                "–ü–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –ø–∞–º—è—Ç–∏",
                metadata={"query": message[:100]}
            )
        
        # –ü–æ–∏—Å–∫ –≤ –ø–∞–º—è—Ç–∏ –∞–≥–µ–Ω—Ç–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å agent_id –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ)
        relevant_memories = []
        agent_id = metadata.get("agent_id") or context.metadata.get("agent_id")
        
        if agent_id:
            try:
                # –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –≤ –ø–∞–º—è—Ç–∏
                relevant_memories = await memory_service.search_memories_vector(
                    agent_id=agent_id,
                    query_text=message,
                    limit=5,
                    similarity_threshold=0.6,
                    combine_with_text_search=True
                )
                logger.debug(f"Found {len(relevant_memories)} relevant memories")
            except Exception as e:
                logger.warning(f"Memory search failed: {e}", exc_info=True)
        
        # –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω—ã —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        if relevant_memories:
            memory_context = "\n\n–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –ø–∞–º—è—Ç–∏:\n"
            for i, memory in enumerate(relevant_memories[:3], 1):  # –ë–µ—Ä–µ–º —Ç–æ–ø-3
                memory_context += f"{i}. {memory.summary or str(memory.content)}\n"
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∫ –∑–∞–ø—Ä–æ—Å—É
            enhanced_message = f"{message}\n\n{memory_context}"
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è LLM
            result = await self._handle_simple_question(enhanced_message, context)
            result.metadata = result.metadata or {}
            result.metadata["memories_used"] = len(relevant_memories)
            result.metadata["memory_search"] = True
            return result
        
        # –ï—Å–ª–∏ –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–π –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º WebSearchTool –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –æ–¥–æ–±—Ä–µ–Ω–∏—è —á–µ—Ä–µ–∑ AdaptiveApprovalService
        from uuid import UUID

        from app.models.approval import ApprovalRequestType
        from app.services.adaptive_approval_service import \
            AdaptiveApprovalService
        from app.services.approval_service import ApprovalService
        from app.services.tool_service import ToolService
        from app.tools.web_search_tool import WebSearchTool

        # –°–æ–∑–¥–∞–µ–º —Ñ–∏–∫—Ç–∏–≤–Ω—ã–π –ø–ª–∞–Ω –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –æ–¥–æ–±—Ä–µ–Ω–∏—è (–¥–ª—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤)
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∏—Å–∫ –¥–ª—è –≤–µ–±-–ø–æ–∏—Å–∫–∞
        adaptive_approval = AdaptiveApprovalService(context.db)
        
        # –î–ª—è –≤–µ–±-–ø–æ–∏—Å–∫–∞ —Å—á–∏—Ç–∞–µ–º —Ä–∏—Å–∫ —Å—Ä–µ–¥–Ω–∏–º (–º–æ–∂–µ—Ç —Ç—Ä–µ–±–æ–≤–∞—Ç—å –æ–¥–æ–±—Ä–µ–Ω–∏—è)
        web_search_risk = 0.5  # –°—Ä–µ–¥–Ω–∏–π —Ä–∏—Å–∫ –¥–ª—è –≤–µ–±-–ø–æ–∏—Å–∫–∞
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Ç—Ä–µ–±—É–µ—Ç—Å—è –ª–∏ –æ–¥–æ–±—Ä–µ–Ω–∏–µ
        # –î–ª—è –≤–µ–±-–ø–æ–∏—Å–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É
        requires_approval = web_search_risk >= 0.4  # MEDIUM_RISK_THRESHOLD
        
        if requires_approval:
            # –°–æ–∑–¥–∞–µ–º –∑–∞–ø—Ä–æ—Å –Ω–∞ –æ–¥–æ–±—Ä–µ–Ω–∏–µ
            approval_service = ApprovalService(context.db)
            approval = approval_service.create_approval_request(
                request_type=ApprovalRequestType.EXECUTION_STEP,
                request_data={
                    "query": message,
                    "search_type": "information_query",
                    "action": "web_search"
                },
                task_id=metadata.get("task_id"),
                risk_assessment={
                    "risk_level": web_search_risk,
                    "reason": "web_search_medium_risk"
                },
                recommendation=f"–¢—Ä–µ–±—É–µ—Ç—Å—è –æ–¥–æ–±—Ä–µ–Ω–∏–µ –¥–ª—è –≤–µ–±-–ø–æ–∏—Å–∫–∞: {message[:100]}"
            )
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –æ–∂–∏–¥–∞–Ω–∏–µ–º –æ–¥–æ–±—Ä–µ–Ω–∏—è
            result = OrchestrationResult(
                response=f"–¢—Ä–µ–±—É–µ—Ç—Å—è –æ–¥–æ–±—Ä–µ–Ω–∏–µ –¥–ª—è –≤–µ–±-–ø–æ–∏—Å–∫–∞: {message[:100]}...",
                model="unknown",
                task_type="information_query",
                metadata={
                    "approval_required": True,
                    "approval_id": str(approval.id),
                    "search_type": "web",
                    "query": message,
                    "requires_approval": True
                }
            )
            return result
        
        # –ï—Å–ª–∏ –æ–¥–æ–±—Ä–µ–Ω–∏–µ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è –∏–ª–∏ —É–∂–µ –ø–æ–ª—É—á–µ–Ω–æ, –≤—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
        try:
            # –ü–æ–ª—É—á–∞–µ–º –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º WebSearchTool
            tool_service = ToolService(context.db)
            
            # –ò—â–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π WebSearchTool
            web_search_tools = tool_service.search_tools(
                name="WebSearchTool",
                category="web_search"
            )
            
            if web_search_tools:
                tool_data = web_search_tools[0]
            else:
                # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω
                tool_data = tool_service.create_tool(
                    name="WebSearchTool",
                    description="–ü–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ",
                    category="web_search",
                    code="",  # WebSearchTool —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω –∫–∞–∫ –∫–ª–∞—Å—Å
                    status="active"
                )
            
            # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä WebSearchTool
            web_search_tool = WebSearchTool(
                tool_id=tool_data.id,
                tool_service=tool_service
            )
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
            search_result = await web_search_tool.execute(
                query=message,
                max_results=5
            )
            
            if search_result.get("status") == "success":
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –≤ –ø–∞–º—è—Ç—å
                try:
                    if agent_id:
                        from uuid import UUID
                        agent_uuid = UUID(agent_id) if isinstance(agent_id, str) else agent_id
                        search_results = search_result.get("result", {}).get("results", [])
                        for result_item in search_results[:3]:  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ø-3
                            await memory_service.save_memory_async(
                                agent_id=agent_uuid,
                                memory_type="episodic",
                                content={
                                    "snippet": result_item.get("snippet", ""),
                                    "title": result_item.get("title", ""),
                                    "url": result_item.get("url", "")
                                },
                                summary=f"–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞: {result_item.get('title', '')}",
                                tags=["web_search"],
                                source="web_search",
                                generate_embedding=True
                            )
                except Exception as e:
                    logger.warning(f"Failed to save search results to memory: {e}", exc_info=True)
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø–æ–∏—Å–∫–∞
                search_results = search_result.get("result", {}).get("results", [])
                results_text = "\n\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ:\n\n"
                for i, result_item in enumerate(search_results, 1):
                    results_text += f"{i}. {result_item.get('title', 'N/A')}\n"
                    results_text += f"   {result_item.get('snippet', '')}\n"
                    if result_item.get('url'):
                        results_text += f"   URL: {result_item.get('url')}\n"
                    results_text += "\n"
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –æ—Ç–≤–µ—Ç–∞ LLM
                enhanced_message = f"{message}\n\n{results_text}"
                result = await self._handle_simple_question(enhanced_message, context)
                result.metadata = result.metadata or {}
                result.metadata["web_search"] = True
                result.metadata["search_results_count"] = len(search_results)
                result.metadata["search_query"] = message
                return result
            else:
                # –ï—Å–ª–∏ –ø–æ–∏—Å–∫ –Ω–µ —É–¥–∞–ª—Å—è, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—ã–π –æ—Ç–≤–µ—Ç
                logger.warning(f"Web search failed: {search_result.get('message')}")
                return await self._handle_simple_question(message, context)
                
        except Exception as e:
            logger.error(
                f"Error executing web search: {e}",
                exc_info=True,
                extra={
                    "query": message,
                    "agent_id": str(agent_id) if agent_id else None
                }
            )
            # Fallback to simple question if web search fails
            return await self._handle_simple_question(message, context)
    
    async def _handle_code_generation(
        self,
        message: str,
        context: ExecutionContext,
        metadata: Dict[str, Any],
        model: Optional[str] = None,
        server_id: Optional[str] = None
    ) -> OrchestrationResult:
        """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –∑–∞–ø—Ä–æ—Å –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∫–æ–¥–∞"""
        logger.debug("Handling code generation request")
        
        # –ü–æ–ª—É—á–∏—Ç—å WorkflowEngine –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–µ—Å–ª–∏ –±—ã–ª —Å–æ–∑–¥–∞–Ω)
        workflow_engine = getattr(context, 'workflow_engine', None)
        
        planning_start = time.time()
        planning_prompt_id = None
        
        # –ü–æ–ª—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –∏ server_id –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏ –∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏
        selected_model, selected_server = self._select_model_and_server(
            context.db, "code_generation", model, server_id
        )
        
        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å –∏ server_id –≤ metadata –∏ context –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –≤ ExecutionService
        if not metadata:
            metadata = {}
        metadata["model"] = selected_model
        metadata["server_id"] = str(selected_server.id)
        metadata["server_url"] = selected_server.get_api_url()
        
        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ context.metadata –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –≤ ExecutionService
        if not context.metadata:
            context.metadata = {}
        context.metadata["model"] = selected_model
        context.metadata["server_id"] = str(selected_server.id)
        context.metadata["server_url"] = selected_server.get_api_url()
        
        # –ü–æ–ª—É—á–∏—Ç—å –ø—Ä–æ–º–ø—Ç –¥–ª—è –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
        if context.prompt_manager:
            try:
                planning_prompt = await context.prompt_manager.get_prompt_for_stage("planning")
                if planning_prompt:
                    planning_prompt_id = planning_prompt.id
            except Exception as e:
                logger.debug(f"Could not get planning prompt: {e}")
        
        # –°–æ–∑–¥–∞—Ç—å –∑–∞–¥–∞—á—É
        task = Task(
            description=message,
            status=TaskStatus.PENDING
        )
        context.db.add(task)
        context.db.commit()
        context.db.refresh(task)
        
        # –ü–æ–ª—É—á–∏—Ç—å PlanningService —á–µ—Ä–µ–∑ ExecutionContext
        from app.services.planning_service import PlanningService
        planning_service = PlanningService(context)
        
        # –ü–µ—Ä–µ—Ö–æ–¥ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–µ PLANNING
        if workflow_engine:
            workflow_engine.transition_to(
                WorkflowState.PLANNING,
                "–ù–∞—á–∞–ª–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–ª–∞–Ω–∞",
                metadata={"task_id": str(task.id)}
            )
        
        # –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–ª–∞–Ω
        plan = None
        planning_success = False
        try:
            plan = await planning_service.generate_plan(
                task_id=task.id,
                task_description=message,
                context=metadata
            )
            planning_success = plan is not None and plan.status == "approved"
            
            # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å AdaptiveApprovalService –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –æ–¥–æ–±—Ä–µ–Ω–∏—è
            from app.services.adaptive_approval_service import \
                AdaptiveApprovalService
            adaptive_approval = AdaptiveApprovalService(context)
            
            # Get task autonomy level
            task_autonomy_level = None
            if plan and plan.task_id:
                from app.models.task import Task
                task = context.db.query(Task).filter(Task.id == plan.task_id).first()
                if task:
                    task_autonomy_level = task.autonomy_level
            
            requires_approval, approval_metadata = adaptive_approval.should_require_approval(
                plan=plan,
                agent_id=None,  # –ú–æ–∂–Ω–æ –ø–µ—Ä–µ–¥–∞—Ç—å agent_id –µ—Å–ª–∏ –∏–∑–≤–µ—Å—Ç–µ–Ω
                task_risk_level=None,
                task_autonomy_level=task_autonomy_level
            )
            
            # –û–±–Ω–æ–≤–∏—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ workflow –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å—Ç–∞—Ç—É—Å–∞ –ø–ª–∞–Ω–∞ –∏ —Ä–µ—à–µ–Ω–∏—è –æ–± –æ–¥–æ–±—Ä–µ–Ω–∏–∏
            if workflow_engine:
                if requires_approval and plan.status == "draft":
                    workflow_engine.transition_to(
                        WorkflowState.APPROVAL_PENDING,
                        f"–ü–ª–∞–Ω —Å–æ–∑–¥–∞–Ω, —Ç—Ä–µ–±—É–µ—Ç—Å—è –æ–¥–æ–±—Ä–µ–Ω–∏–µ ({approval_metadata.get('reason', 'unknown')})",
                        metadata={
                            "plan_id": str(plan.id),
                            "approval_metadata": approval_metadata
                        }
                    )
                elif plan.status == "approved":
                    workflow_engine.transition_to(
                        WorkflowState.APPROVED,
                        "–ü–ª–∞–Ω –æ–¥–æ–±—Ä–µ–Ω, –≥–æ—Ç–æ–≤ –∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é",
                        metadata={"plan_id": str(plan.id), "steps_count": len(plan.steps) if plan.steps else 0}
                    )
                elif not requires_approval and plan.status == "draft":
                    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–¥–æ–±—Ä—è–µ–º –µ—Å–ª–∏ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è –æ–¥–æ–±—Ä–µ–Ω–∏–µ
                    workflow_engine.transition_to(
                        WorkflowState.APPROVED,
                        "–ü–ª–∞–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–¥–æ–±—Ä–µ–Ω (–Ω–∏–∑–∫–∏–π —Ä–∏—Å–∫, –≤—ã—Å–æ–∫–æ–µ –¥–æ–≤–µ—Ä–∏–µ)",
                        metadata={
                            "plan_id": str(plan.id),
                            "auto_approved": True,
                            "approval_metadata": approval_metadata
                        }
                    )
                    # –û–±–Ω–æ–≤–∏—Ç—å —Å—Ç–∞—Ç—É—Å –ø–ª–∞–Ω–∞ –≤ –ë–î
                    plan.status = "approved"
                    try:
                        # Keep lifecycle metadata consistent
                        from datetime import datetime as _dt
                        plan.approved_at = _dt.utcnow()
                    except Exception:
                        pass
                    context.db.commit()
        except Exception as e:
            logger.error(f"Plan generation failed: {e}", exc_info=True)
            workflow_engine = getattr(context, 'workflow_engine', None)
            if workflow_engine:
                workflow_engine.mark_failed(
                    error=f"Plan generation failed: {str(e)}",
                    error_details={"task_id": str(task.id), "stage": "planning"}
                )
        
        # –ó–∞–ø–∏—Å–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–º–ø—Ç–∞ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
        if context.prompt_manager and planning_prompt_id:
            planning_time_ms = (time.time() - planning_start) * 1000
            await context.prompt_manager.record_prompt_usage(
                prompt_id=planning_prompt_id,
                success=planning_success,
                execution_time_ms=planning_time_ms,
                stage="planning"
            )
        
        if plan and plan.status == "approved":
            # –ü–µ—Ä–µ—Ö–æ–¥ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–µ EXECUTING
            if workflow_engine:
                workflow_engine.transition_to(
                    WorkflowState.EXECUTING,
                    "–ù–∞—á–∞–ª–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø–ª–∞–Ω–∞",
                    metadata={"plan_id": str(plan.id)}
                )
            
            execution_start = time.time()
            execution_prompt_id = None
            
            # –ü–æ–ª—É—á–∏—Ç—å –ø—Ä–æ–º–ø—Ç –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
            if context.prompt_manager:
                try:
                    execution_prompt = await context.prompt_manager.get_prompt_for_stage("execution")
                    if execution_prompt:
                        execution_prompt_id = execution_prompt.id
                except Exception as e:
                    logger.debug(f"Could not get execution prompt: {e}")
            
            # –ü–æ–ª—É—á–∏—Ç—å ExecutionService —á–µ—Ä–µ–∑ ExecutionContext
            from app.services.execution_service import ExecutionService
            execution_service = ExecutionService(context)
            
            # –í—ã–ø–æ–ª–Ω–∏—Ç—å –ø–ª–∞–Ω
            execution_success = False
            try:
                executed_plan = await execution_service.execute_plan(plan.id)
                execution_success = executed_plan.status == "completed"
                
                # –û–±–Ω–æ–≤–∏—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ workflow
                if workflow_engine:
                    if execution_success:
                        workflow_engine.mark_completed(result=result_text[:200] if 'result_text' in locals() else None)
                    else:
                        workflow_engine.mark_failed(
                            error=f"Plan execution failed: {executed_plan.status}",
                            error_details={"plan_id": str(plan.id), "final_status": executed_plan.status}
                        )
                
                # –ò–∑–≤–ª–µ—á—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                result_text = self._extract_plan_results(executed_plan)
                
                # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –≤ –ø–∞–º—è—Ç—å —á–µ—Ä–µ–∑ MemoryService
                try:
                    from app.services.memory_service import MemoryService
                    memory_service = MemoryService(context)
                    
                    # –ü–æ–ª—É—á–∞–µ–º agent_id –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–ª–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
                    agent_id = metadata.get("agent_id") or context.metadata.get("agent_id")
                    
                    if agent_id:
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–∞–º—è—Ç—å –æ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω–æ–π –∑–∞–¥–∞—á–µ
                        memory_service.save_memory(
                            agent_id=agent_id,
                            memory_type="execution",
                            content={
                                "task": message,
                                "plan_id": str(plan.id),
                                "status": executed_plan.status,
                                "result": result_text[:500] if result_text else None
                            },
                            summary=f"–í—ã–ø–æ–ª–Ω–µ–Ω–∞ –∑–∞–¥–∞—á–∞: {message[:100]}",
                            importance=0.7 if execution_success else 0.5,
                            tags=["execution", "code_generation", "success" if execution_success else "failure"]
                        )
                        logger.debug(f"Saved execution memory for agent {agent_id}")
                except Exception as e:
                    logger.warning(f"Failed to save execution memory: {e}", exc_info=True)
                
                # –ó–∞–ø–∏—Å–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–º–ø—Ç–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
                if context.prompt_manager and execution_prompt_id:
                    execution_time_ms = (time.time() - execution_start) * 1000
                    await context.prompt_manager.record_prompt_usage(
                        prompt_id=execution_prompt_id,
                        success=execution_success,
                        execution_time_ms=execution_time_ms,
                        stage="execution"
                    )
                
                # –ü–æ–ª—É—á–∏—Ç—å —Ä–µ–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–µ—Ä–µ–¥–∞–Ω–Ω—É—é
                actual_model = metadata.get("model") or context.metadata.get("model") or "planning+execution"
                
                return OrchestrationResult(
                    response=result_text,
                    model=selected_model,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –≤–º–µ—Å—Ç–æ "planning+execution"
                    task_type="code_generation",
                    metadata={
                        "plan_id": str(plan.id), 
                        "task_id": str(task.id),
                        "used_model": selected_model,
                        "execution_method": "planning+execution"
                    }
                )
            except Exception as e:
                logger.error(f"Plan execution failed: {e}", exc_info=True)
                if workflow_engine:
                    workflow_engine.mark_failed(
                        error=f"Plan execution exception: {str(e)}",
                        error_details={"plan_id": str(plan.id), "exception_type": type(e).__name__}
                    )
                
                # –ó–∞–ø–∏—Å–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –Ω–µ—É–¥–∞—á–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
                if context.prompt_manager and execution_prompt_id:
                    execution_time_ms = (time.time() - execution_start) * 1000
                    await context.prompt_manager.record_prompt_usage(
                        prompt_id=execution_prompt_id,
                        success=False,
                        execution_time_ms=execution_time_ms,
                        stage="execution"
                    )
                
                raise
        else:
            # Fallback –∫ –ø—Ä–æ—Å—Ç–æ–º—É LLM
            logger.warning(f"Plan generation failed, falling back to direct LLM")
            return await self._handle_simple_question(message, context, task_type="code_generation")
    
    async def _handle_complex_task(
        self,
        message: str,
        context: ExecutionContext,
        metadata: Dict[str, Any]
    ) -> OrchestrationResult:
        """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å —Å–ª–æ–∂–Ω—É—é –∑–∞–¥–∞—á—É - –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ + –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ + —Ä–µ—Ñ–ª–µ–∫—Å–∏—è"""
        logger.debug("Handling complex task")
        
        # –ü–æ–ª—É—á–∏—Ç—å WorkflowEngine –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        workflow_engine = getattr(context, 'workflow_engine', None)
        if workflow_engine:
            workflow_engine.transition_to(
                WorkflowState.PLANNING,
                "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ª–æ–∂–Ω–æ–π –∑–∞–¥–∞—á–∏ —Å —Ä–µ—Ñ–ª–µ–∫—Å–∏–µ–π",
                metadata={"task": message[:100]}
            )
        
        # –°–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω—è–µ–º –∫–∞–∫ code_generation
        result = await self._handle_code_generation(message, context, metadata, None, None)
        
        # –ü–æ—Å–ª–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –¥–æ–±–∞–≤–ª—è–µ–º —Ä–µ—Ñ–ª–µ–∫—Å–∏—é —á–µ—Ä–µ–∑ ReflectionService
        from app.services.reflection_service import ReflectionService
        
        reflection_service = ReflectionService(context)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        try:
            # –ï—Å–ª–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –±—ã–ª–æ –Ω–µ—É–¥–∞—á–Ω—ã–º, –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –æ—à–∏–±–∫—É
            if result.metadata and result.metadata.get("execution_success") is False:
                error_info = result.metadata.get("error", "Unknown error")
                
                # –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–∫–∏ —á–µ—Ä–µ–∑ ReflectionService
                analysis = await reflection_service.analyze_failure(
                    task_description=message,
                    error=error_info,
                    context=result.metadata
                )
                
                # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è
                if analysis and analysis.get("root_cause"):
                    fix = await reflection_service.generate_fix(
                        task_description=message,
                        error=error_info,
                        analysis=analysis
                    )
                    
                    if fix and fix.get("suggested_fix"):
                        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –ø–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—é –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É
                        result.response += f"\n\nüí° –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –ø–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—é:\n{fix.get('suggested_fix')}"
                        result.metadata["reflection_analysis"] = analysis
                        result.metadata["reflection_fix"] = fix
                        
                        logger.info("Reflection analysis completed", extra={
                            "workflow_id": context.workflow_id,
                            "root_cause": analysis.get("root_cause")
                        })
            
            # –ê–Ω–∞–ª–∏–∑ —É—Å–ø–µ—à–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è
            elif result.metadata and result.metadata.get("execution_success") is True:
                # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –∞–Ω–∞–ª–∏–∑ —É—Å–ø–µ—à–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è
                logger.debug("Task completed successfully, skipping reflection")
                
        except Exception as e:
            logger.warning(f"Reflection analysis failed: {e}", exc_info=True)
            # –ù–µ –ø—Ä–µ—Ä—ã–≤–∞–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ, –µ—Å–ª–∏ —Ä–µ—Ñ–ª–µ–∫—Å–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å
        
        # –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è MetaLearningService –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        from app.services.meta_learning_service import MetaLearningService
        
        meta_learning_service = MetaLearningService(context)
        
        try:
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è
            # –ü–æ–ª—É—á–∞–µ–º agent_id –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            agent_id = metadata.get("agent_id") or context.metadata.get("agent_id")
            agent_uuid = None
            if agent_id:
                try:
                    from uuid import UUID
                    agent_uuid = UUID(agent_id) if isinstance(agent_id, str) else agent_id
                except (ValueError, TypeError):
                    pass
            
            # Use synchronous analysis to avoid awaiting a coroutine in this context
            patterns = meta_learning_service.analyze_execution_patterns_sync(
                agent_id=agent_uuid,
                time_range_days=1  # –ê–Ω–∞–ª–∏–∑ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π –¥–µ–Ω—å
            )
            
            if patterns and patterns.get("total_executions", 0) > 0:
                logger.debug(f"Meta-learning analysis completed: {patterns.get('total_executions')} executions analyzed")
                result.metadata = result.metadata or {}
                result.metadata["meta_learning_patterns"] = patterns
                
        except Exception as e:
            logger.warning(f"Meta-learning analysis failed: {e}", exc_info=True)
        
        return result
    
    async def _handle_planning_only(
        self,
        message: str,
        context: ExecutionContext,
        metadata: Dict[str, Any],
        model: Optional[str] = None,
        server_id: Optional[str] = None
    ) -> OrchestrationResult:
        """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –∑–∞–ø—Ä–æ—Å —Ç–æ–ª—å–∫–æ –Ω–∞ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ (–±–µ–∑ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è)"""
        logger.debug("Handling planning-only request")
        
        # –ü–æ–ª—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –∏ server_id –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        selected_model, selected_server = self._select_model_and_server(
            context.db, "planning", model, server_id
        )
        
        planning_start = time.time()
        planning_prompt_id = None
        
        # –ü–æ–ª—É—á–∏—Ç—å –ø—Ä–æ–º–ø—Ç –¥–ª—è –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
        if context.prompt_manager:
            try:
                planning_prompt = await context.prompt_manager.get_prompt_for_stage("planning")
                if planning_prompt:
                    planning_prompt_id = planning_prompt.id
            except Exception as e:
                logger.debug(f"Could not get planning prompt: {e}")
        
        # –°–æ–∑–¥–∞—Ç—å –∑–∞–¥–∞—á—É
        task = Task(
            description=message,
            status=TaskStatus.PENDING
        )
        context.db.add(task)
        context.db.commit()
        context.db.refresh(task)
        
        # –ü–æ–ª—É—á–∏—Ç—å PlanningService —á–µ—Ä–µ–∑ ExecutionContext
        from app.services.planning_service import PlanningService
        planning_service = PlanningService(context)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–ª–∞–Ω
        plan = None
        planning_success = False
        try:
            plan = await planning_service.generate_plan(
                task_id=task.id,
                task_description=message,
                context=metadata
            )
            planning_success = plan is not None
        except Exception as e:
            logger.error(f"Plan generation failed: {e}", exc_info=True)
        
        # –ó–∞–ø–∏—Å–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–º–ø—Ç–∞ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
        if context.prompt_manager and planning_prompt_id:
            planning_time_ms = (time.time() - planning_start) * 1000
            await context.prompt_manager.record_prompt_usage(
                prompt_id=planning_prompt_id,
                success=planning_success,
                execution_time_ms=planning_time_ms,
                stage="planning"
            )
        
        if plan:
            # –í–µ—Ä–Ω—É—Ç—å –æ–ø–∏—Å–∞–Ω–∏–µ –ø–ª–∞–Ω–∞
            steps = plan.steps or []
            plan_description = f"–°–æ–∑–¥–∞–Ω –ø–ª–∞–Ω –∏–∑ {len(steps)} —à–∞–≥–æ–≤:\n\n"
            for i, step in enumerate(steps, 1):
                step_desc = step.get("description", "–ë–µ–∑ –æ–ø–∏—Å–∞–Ω–∏—è")
                plan_description += f"{i}. {step_desc}\n"
            
            return OrchestrationResult(
                response=plan_description,
                model=selected_model,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –≤–º–µ—Å—Ç–æ "planning"
                task_type="planning_only",
                metadata={"plan_id": str(plan.id), "task_id": str(task.id), "used_model": selected_model}
            )
        else:
            return OrchestrationResult(
                response="–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –ø–ª–∞–Ω",
                model=selected_model,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
                task_type="planning_only"
            )
    
    def _select_model_and_server(
        self,
        db: Session,
        task_type: Optional[str] = None,
        model: Optional[str] = None,
        server_id: Optional[str] = None
    ) -> Tuple[Optional[str], Optional[Any]]:
        """–í—ã–±—Ä–∞—Ç—å –º–æ–¥–µ–ª—å –∏ —Å–µ—Ä–≤–µ—Ä –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞"""
        selected_model = None
        selected_server = None
        
        # PRIORITY 1: –ï—Å–ª–∏ server_id —É–∫–∞–∑–∞–Ω
        if server_id and server_id.strip():
            selected_server = OllamaService.get_server_by_id(db, server_id.strip())
            if not selected_server:
                raise ValueError(f"Server {server_id} not found")
        
        # PRIORITY 2: –ï—Å–ª–∏ model —É–∫–∞–∑–∞–Ω–∞, –Ω–∞–π—Ç–∏ —Å–µ—Ä–≤–µ—Ä
        if model and model.strip():
            selected_model = model.strip()
            if not selected_server:
                # –ò—â–µ–º —Å–µ—Ä–≤–µ—Ä —Å —ç—Ç–æ–π –º–æ–¥–µ–ª—å—é
                all_servers = OllamaService.get_all_active_servers(db)
                for server in all_servers:
                    models = OllamaService.get_models_for_server(db, str(server.id))
                    if any(m.model_name == selected_model for m in models):
                        selected_server = server
                        break
        
        # PRIORITY 3: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ task_type
        if not selected_model or not selected_server:
            model_selector = ModelSelector(db)
            
            if task_type == "code_generation":
                model_obj = model_selector.get_code_model(selected_server)
            elif task_type in ["planning", "reasoning"]:
                model_obj = model_selector.get_planning_model(selected_server)
            else:
                # –û–±—â–∞—è –º–æ–¥–µ–ª—å - –∏—â–µ–º chat/general –º–æ–¥–µ–ª–∏
                if selected_server:
                    models = OllamaService.get_models_for_server(db, str(selected_server.id))
                    # –§–∏–ª—å—Ç—Ä—É–µ–º embedding –º–æ–¥–µ–ª–∏ –∏ –∏—â–µ–º chat/general
                    chat_models = [
                        m for m in models
                        if m.is_active 
                        and not any(cap in ['embedding'] for cap in (m.capabilities or []))
                        and ("embedding" not in m.model_name.lower() and "embed" not in m.model_name.lower())
                    ]
                    # –ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º –º–æ–¥–µ–ª–∏ —Å chat/general capabilities
                    preferred = [m for m in chat_models if any(cap in ['chat', 'general'] for cap in (m.capabilities or []))]
                    model_obj = preferred[0] if preferred else (chat_models[0] if chat_models else None)
                else:
                    default_server = OllamaService.get_default_server(db)
                    if default_server:
                        models = OllamaService.get_models_for_server(db, str(default_server.id))
                        # –§–∏–ª—å—Ç—Ä—É–µ–º embedding –º–æ–¥–µ–ª–∏
                        chat_models = [
                            m for m in models
                            if m.is_active 
                            and not any(cap in ['embedding'] for cap in (m.capabilities or []))
                            and ("embedding" not in m.model_name.lower() and "embed" not in m.model_name.lower())
                        ]
                        preferred = [m for m in chat_models if any(cap in ['chat', 'general'] for cap in (m.capabilities or []))]
                        model_obj = preferred[0] if preferred else (chat_models[0] if chat_models else None)
                        selected_server = default_server
                    else:
                        model_obj = None
            
            if model_obj:
                selected_model = model_obj.model_name
                if not selected_server:
                    selected_server = model_selector.get_server_for_model(model_obj)
        
        # PRIORITY 4: Fallback - –ª—é–±–æ–π –¥–æ—Å—Ç—É–ø–Ω—ã–π —Å–µ—Ä–≤–µ—Ä –∏ –º–æ–¥–µ–ª—å
        if not selected_model or not selected_server:
            all_servers = OllamaService.get_all_active_servers(db)
            for server in all_servers:
                models = OllamaService.get_models_for_server(db, str(server.id))
                # –§–∏–ª—å—Ç—Ä—É–µ–º embedding –º–æ–¥–µ–ª–∏ –ø–æ capabilities –ò –ø–æ –∏–º–µ–Ω–∏
                chat_models = [
                    m for m in models
                    if m.is_active 
                    and m.model_name
                    and not any(cap.lower() in ['embedding', 'embed'] for cap in (m.capabilities or []))
                    and "embedding" not in m.model_name.lower()
                    and "embed" not in m.model_name.lower()
                ]
                if chat_models:
                    selected_server = server
                    preferred = [
                        m for m in chat_models
                        if any(cap.lower() in ['chat', 'general'] for cap in (m.capabilities or []))
                    ]
                    if preferred:
                        selected_model = preferred[0].model_name
                    else:
                        selected_model = chat_models[0].model_name
                    break
        
        if not selected_model or not selected_server:
            raise ValueError("No available model or server found")
        
        return selected_model, selected_server
    
    async def _get_system_prompt(self, context: ExecutionContext) -> Optional[str]:
        """–ü–æ–ª—É—á–∏—Ç—å system prompt –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞"""
        try:
            # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å PromptManager –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            if context.prompt_manager:
                prompt = await context.prompt_manager.get_prompt_for_stage("planning")
                if prompt:
                    return prompt.prompt_text
            else:
                # –ï—Å–ª–∏ PromptManager –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, —Å–æ–∑–¥–∞—Ç—å –µ–≥–æ
                from app.core.prompt_manager import PromptManager
                context.set_prompt_manager(PromptManager(context))
                prompt = await context.prompt_manager.get_prompt_for_stage("planning")
                if prompt:
                    return prompt.prompt_text
        except Exception as e:
            logger.warning(f"Failed to get system prompt: {e}")
        
        return None
    
    def _extract_plan_results(self, plan) -> str:
        """–ò–∑–≤–ª–µ—á—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø–ª–∞–Ω–∞"""
        if not plan:
            return "–ü–ª–∞–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω"
        
        steps = plan.steps or []
        results = []
        
        for i, step in enumerate(steps, 1):
            step_result = step.get("result")
            step_output = step.get("output")
            
            if step_output:
                results.append(f"–®–∞–≥ {i}: {step_output}")
            elif step_result:
                if isinstance(step_result, dict):
                    result_text = step_result.get("output") or step_result.get("result") or str(step_result)
                else:
                    result_text = str(step_result)
                
                if result_text and result_text != "None":
                    results.append(f"–®–∞–≥ {i}: {result_text}")
        
        if results:
            return "\n\n".join(results)
        
        if plan.status == "completed":
            return f"–ü–ª–∞–Ω –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ. –í—ã–ø–æ–ª–Ω–µ–Ω–æ —à–∞–≥–æ–≤: {len(steps)}"
        
        return f"–ü–ª–∞–Ω –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è. –°—Ç–∞—Ç—É—Å: {plan.status}"
