"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ —É–¥–∞–ª–µ–Ω–∏—è –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–≥–æ –∫–æ–¥–∞
–≠—Ç–∞–ø 9.1.2: –£–¥–∞–ª–∏—Ç—å –∏–ª–∏ –ø–µ—Ä–µ–º–µ—Å—Ç–∏—Ç—å –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π –∫–æ–¥
"""
import sys
import re
import ast
from pathlib import Path
from typing import List, Dict, Set
import shutil

# Add backend to path
backend_dir = Path(__file__).parent.parent
sys.path.insert(0, str(backend_dir))

from app.core.logging_config import LoggingConfig

logger = LoggingConfig.get_logger(__name__)


class CodeCleaner:
    """–û—á–∏—Å—Ç–∫–∞ –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–≥–æ –∫–æ–¥–∞"""
    
    def __init__(self, dry_run: bool = True):
        self.backend_dir = backend_dir
        self.dry_run = dry_run
        self.removed_imports = []
        self.removed_functions = []
        self.moved_files = []
        
    def remove_unused_imports(self, file_path: Path, unused_imports: List[str]) -> bool:
        """–£–¥–∞–ª–∏—Ç—å –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –∏–º–ø–æ—Ä—Ç—ã –∏–∑ —Ñ–∞–π–ª–∞"""
        try:
            content = file_path.read_text(encoding="utf-8")
            lines = content.split('\n')
            new_lines = []
            i = 0
            
            while i < len(lines):
                line = lines[i]
                stripped = line.strip()
                
                # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ –∏–º–ø–æ—Ä—Ç–æ–º, –∫–æ—Ç–æ—Ä—ã–π –Ω—É–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å
                should_remove = False
                for unused in unused_imports:
                    if re.match(rf'^import\s+{re.escape(unused)}\s*$', stripped) or \
                       re.match(rf'^from\s+{re.escape(unused)}\s+import', stripped):
                        should_remove = True
                        break
                
                if should_remove:
                    # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å —ç—Ç—É —Å—Ç—Ä–æ–∫—É
                    self.removed_imports.append({
                        "file": str(file_path.relative_to(self.backend_dir)),
                        "import": unused,
                        "line": i + 1
                    })
                    i += 1
                    continue
                
                new_lines.append(line)
                i += 1
            
            if not self.dry_run and new_lines != lines:
                file_path.write_text('\n'.join(new_lines), encoding="utf-8")
                return True
            
            return False
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –∏–º–ø–æ—Ä—Ç–æ–≤ –∏–∑ {file_path}: {e}")
            return False
    
    def find_unused_test_files(self) -> List[Path]:
        """–ù–∞–π—Ç–∏ –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —Ç–µ—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã"""
        unused = []
        tests_dir = self.backend_dir / "tests"
        
        if not tests_dir.exists():
            return unused
        
        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –µ—Å—Ç—å –ª–∏ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã
        for test_file in tests_dir.rglob("test_*.py"):
            # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å conftest.py –∏ __init__.py
            if test_file.name in ["conftest.py", "__init__.py"]:
                continue
            
            # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç—Å—è –ª–∏ —ç—Ç–æ—Ç —Ñ–∞–π–ª –≥–¥–µ-—Ç–æ
            test_module = str(test_file.relative_to(self.backend_dir)).replace('\\', '/').replace('/', '.').replace('.py', '')
            
            # –ü–æ–∏—Å–∫ –∏–º–ø–æ—Ä—Ç–æ–≤ —ç—Ç–æ–≥–æ –º–æ–¥—É–ª—è
            found_import = False
            for py_file in self.backend_dir.rglob("*.py"):
                if py_file == test_file:
                    continue
                
                try:
                    content = py_file.read_text(encoding="utf-8")
                    if test_module in content or test_file.stem in content:
                        found_import = True
                        break
                except:
                    continue
            
            if not found_import:
                # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã–º —Ç–µ—Å—Ç–æ–º
                try:
                    content = test_file.read_text(encoding="utf-8")
                    # –ï—Å–ª–∏ –≤ —Ñ–∞–π–ª–µ –µ—Å—Ç—å if __name__ == "__main__", —Ç–æ —ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –∞–≤—Ç–æ–Ω–æ–º–Ω—ã–π —Ç–µ—Å—Ç
                    if "__main__" not in content and "pytest" not in content:
                        unused.append(test_file)
                except:
                    pass
        
        return unused
    
    def find_duplicate_files(self) -> List[Dict]:
        """–ù–∞–π—Ç–∏ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã"""
        duplicates = []
        file_contents = {}
        
        # –°–æ–±—Ä–∞—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤—Å–µ—Ö Python —Ñ–∞–π–ª–æ–≤
        for py_file in self.backend_dir.rglob("*.py"):
            if "__pycache__" in str(py_file) or "migrations" in str(py_file):
                continue
            
            try:
                content = py_file.read_text(encoding="utf-8")
                # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å (—É–±—Ä–∞—Ç—å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è)
                normalized = '\n'.join(l.strip() for l in content.split('\n') 
                                      if l.strip() and not l.strip().startswith('#'))
                
                if normalized in file_contents:
                    duplicates.append({
                        "original": file_contents[normalized],
                        "duplicate": str(py_file.relative_to(self.backend_dir))
                    })
                else:
                    file_contents[normalized] = str(py_file.relative_to(self.backend_dir))
            except:
                continue
        
        return duplicates
    
    def cleanup(self, audit_results_path: Path) -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω–∏—Ç—å –æ—á–∏—Å—Ç–∫—É –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞—É–¥–∏—Ç–∞"""
        logger.info("–ù–∞—á–∞–ª–æ –æ—á–∏—Å—Ç–∫–∏ –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–≥–æ –∫–æ–¥–∞...")
        
        # –ó–∞–≥—Ä—É–∑–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞—É–¥–∏—Ç–∞
        import json
        with open(audit_results_path, 'r', encoding='utf-8') as f:
            audit_results = json.load(f)
        
        # –£–¥–∞–ª–∏—Ç—å –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –∏–º–ø–æ—Ä—Ç—ã
        logger.info("–£–¥–∞–ª–µ–Ω–∏–µ –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –∏–º–ø–æ—Ä—Ç–æ–≤...")
        for item in audit_results.get("unused_imports", [])[:50]:  # –û–≥—Ä–∞–Ω–∏—á–∏—Ç—å –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
            file_path = self.backend_dir / item["file"]
            if file_path.exists():
                self.remove_unused_imports(file_path, item["imports"])
        
        # –ù–∞–π—Ç–∏ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
        logger.info("–ü–æ–∏—Å–∫ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤...")
        duplicate_files = self.find_duplicate_files()
        
        # –ù–∞–π—Ç–∏ –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —Ç–µ—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã
        logger.info("–ü–æ–∏—Å–∫ –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤...")
        unused_test_files = self.find_unused_test_files()
        
        return {
            "removed_imports": self.removed_imports,
            "duplicate_files": duplicate_files,
            "unused_test_files": [str(f.relative_to(self.backend_dir)) for f in unused_test_files],
            "summary": {
                "removed_imports_count": len(self.removed_imports),
                "duplicate_files_count": len(duplicate_files),
                "unused_test_files_count": len(unused_test_files)
            }
        }


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    import argparse
    
    parser = argparse.ArgumentParser(description="–û—á–∏—Å—Ç–∫–∞ –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–≥–æ –∫–æ–¥–∞")
    parser.add_argument("--dry-run", action="store_true", default=True,
                       help="–¢–æ–ª—å–∫–æ –ø–æ–∫–∞–∑–∞—Ç—å —á—Ç–æ –±—É–¥–µ—Ç —É–¥–∞–ª–µ–Ω–æ, –Ω–µ —É–¥–∞–ª—è—Ç—å")
    parser.add_argument("--execute", action="store_true",
                       help="–í—ã–ø–æ–ª–Ω–∏—Ç—å —É–¥–∞–ª–µ–Ω–∏–µ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é dry-run)")
    args = parser.parse_args()
    
    dry_run = not args.execute
    
    print("=" * 70)
    print(" –û—á–∏—Å—Ç–∫–∞ –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–≥–æ –∫–æ–¥–∞ –ø—Ä–æ–µ–∫—Ç–∞ AARD")
    print("=" * 70 + "\n")
    
    if dry_run:
        print("‚ö†Ô∏è  –†–ï–ñ–ò–ú DRY-RUN: –∏–∑–º–µ–Ω–µ–Ω–∏—è –Ω–µ –±—É–¥—É—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω—ã\n")
    else:
        print("‚ö†Ô∏è  –†–ï–ñ–ò–ú –í–´–ü–û–õ–ù–ï–ù–ò–Ø: –∏–∑–º–µ–Ω–µ–Ω–∏—è –±—É–¥—É—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω—ã!\n")
    
    cleaner = CodeCleaner(dry_run=dry_run)
    
    # –ü—É—Ç—å –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –∞—É–¥–∏—Ç–∞
    audit_results_path = cleaner.backend_dir.parent / "docs" / "TECHNICAL_DEBT.json"
    
    if not audit_results_path.exists():
        print(f"‚ùå –§–∞–π–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞—É–¥–∏—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω: {audit_results_path}")
        print("   –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ: python scripts/code_audit.py")
        sys.exit(1)
    
    try:
        results = cleaner.cleanup(audit_results_path)
        
        print(f"\n‚úÖ –û—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞:")
        summary = results.get("summary", {})
        for key, value in summary.items():
            print(f"   - {key}: {value}")
        
        if dry_run:
            print("\nüí° –î–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π –∑–∞–ø—É—Å—Ç–∏—Ç–µ —Å —Ñ–ª–∞–≥–æ–º --execute")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏: {e}", exc_info=True)
        print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()

