"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏ –≤—Å–µ—Ö –ø–ª–∞–Ω–æ–≤ –ø—Ä–æ–µ–∫—Ç–∞
–°–æ–±–∏—Ä–∞–µ—Ç –ø–ª–∞–Ω—ã –∏–∑ —Ñ–∞–π–ª–æ–≤ –∏ –ë–î, –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏—Ö, –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –µ–¥–∏–Ω—ã–π –ø–ª–∞–Ω
"""
import asyncio
import json
import re
import sys
from collections import defaultdict
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

# Add backend to path
backend_dir = Path(__file__).parent.parent
sys.path.insert(0, str(backend_dir))

from app.core.config import get_settings
from app.core.database import SessionLocal
from app.core.logging_config import LoggingConfig
from app.core.ollama_client import OllamaClient, TaskType
from app.models.plan import Plan
from app.models.task import Task, TaskStatus

logger = LoggingConfig.get_logger(__name__)


class PlanConsolidator:
    """–ö–ª–∞—Å—Å –¥–ª—è –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏ –≤—Å–µ—Ö –ø–ª–∞–Ω–æ–≤ –ø—Ä–æ–µ–∫—Ç–∞"""
    
    def __init__(self):
        self.project_root = backend_dir.parent
        self.plans_dir = self.project_root / ".cursor" / "plans"
        self.archive_dir = self.project_root / "docs" / "archive"
        self.backend_dir = self.project_root / "backend"
        self.ollama_client = OllamaClient()
        self.all_tasks = []
        self.all_plans = []
        
    def collect_file_plans(self) -> List[Dict]:
        """–°–æ–±—Ä–∞—Ç—å –≤—Å–µ –ø–ª–∞–Ω—ã –∏–∑ —Ñ–∞–π–ª–æ–≤"""
        plans = []
        
        # –ü–ª–∞–Ω—ã –∏–∑ .cursor/plans/
        if self.plans_dir.exists():
            for plan_file in self.plans_dir.glob("*.md"):
                try:
                    content = plan_file.read_text(encoding="utf-8")
                    plans.append({
                        "source": "cursor_plans",
                        "file": str(plan_file.relative_to(self.project_root)),
                        "content": content,
                        "name": plan_file.stem,
                        "size": len(content),
                        "modified": datetime.fromtimestamp(plan_file.stat().st_mtime).isoformat()
                    })
                except Exception as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {plan_file}: {e}")
        
        # –ü–ª–∞–Ω—ã –∏–∑ docs/archive/ (—Ç–æ–ª—å–∫–æ —Ñ–∞–π–ª—ã —Å PLAN/ROADMAP –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏)
        if self.archive_dir.exists():
            for plan_file in self.archive_dir.glob("*.md"):
                if any(keyword in plan_file.name.upper() for keyword in ["PLAN", "ROADMAP", "ROAD"]):
                    try:
                        content = plan_file.read_text(encoding="utf-8")
                        plans.append({
                            "source": "archive",
                            "file": str(plan_file.relative_to(self.project_root)),
                            "content": content,
                            "name": plan_file.stem,
                            "size": len(content),
                            "modified": datetime.fromtimestamp(plan_file.stat().st_mtime).isoformat()
                        })
                    except Exception as e:
                        logger.warning(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {plan_file}: {e}")
        
        return plans
    
    def collect_db_plans(self) -> List[Dict]:
        """–°–æ–±—Ä–∞—Ç—å –≤—Å–µ –ø–ª–∞–Ω—ã –∏–∑ –ë–î"""
        db = SessionLocal()
        try:
            plans = db.query(Plan).order_by(Plan.created_at.desc()).all()
            return [{
                "source": "database",
                "id": str(p.id),
                "task_id": str(p.task_id) if p.task_id else None,
                "version": p.version,
                "goal": p.goal,
                "strategy": p.strategy,
                "steps": p.steps,
                "alternatives": p.alternatives,
                "status": p.status,
                "current_step": p.current_step,
                "created_at": p.created_at.isoformat() if p.created_at else None,
                "approved_at": p.approved_at.isoformat() if p.approved_at else None,
            } for p in plans]
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –ø–ª–∞–Ω–æ–≤ –∏–∑ –ë–î: {e}")
            return []
        finally:
            db.close()
    
    def calculate_text_similarity(self, text1: str, text2: str) -> float:
        """–í—ã—á–∏—Å–ª–∏—Ç—å —Å—Ö–æ–∂–µ—Å—Ç—å –¥–≤—É—Ö —Ç–µ–∫—Å—Ç–æ–≤ (–ø—Ä–æ—Å—Ç–∞—è –º–µ—Ç—Ä–∏–∫–∞)"""
        # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç—ã
        text1_lower = text1.lower().strip()
        text2_lower = text2.lower().strip()
        
        # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç—ã –∏–¥–µ–Ω—Ç–∏—á–Ω—ã
        if text1_lower == text2_lower:
            return 1.0
        
        # –í—ã—á–∏—Å–ª–∏—Ç—å –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å–ª–æ–≤
        words1 = set(re.findall(r'\w+', text1_lower))
        words2 = set(re.findall(r'\w+', text2_lower))
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1 & words2
        union = words1 | words2
        
        # Jaccard similarity
        jaccard = len(intersection) / len(union) if union else 0.0
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ–¥—Å—Ç—Ä–æ–∫—É
        if text1_lower in text2_lower or text2_lower in text1_lower:
            substring_bonus = 0.3
        else:
            substring_bonus = 0.0
        
        return min(jaccard + substring_bonus, 1.0)
    
    def deduplicate_tasks(self, tasks: List[Dict], similarity_threshold: float = 0.7) -> List[Dict]:
        """–î–µ–¥—É–ø–ª–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –∑–∞–¥–∞—á–∏ —Å –ø–æ—Ö–æ–∂–∏–º —Ç–µ–∫—Å—Ç–æ–º"""
        if not tasks:
            return tasks
        
        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å –ø–æ—Ö–æ–∂–∏–µ –∑–∞–¥–∞—á–∏
        task_groups = []
        processed = set()
        
        for i, task1 in enumerate(tasks):
            if i in processed:
                continue
            
            # –ù–∞–π—Ç–∏ –ø–æ—Ö–æ–∂–∏–µ –∑–∞–¥–∞—á–∏
            similar_tasks = [task1]
            similar_indices = [i]
            
            for j, task2 in enumerate(tasks[i+1:], start=i+1):
                if j in processed:
                    continue
                
                similarity = self.calculate_text_similarity(
                    task1.get('text', ''),
                    task2.get('text', '')
                )
                
                if similarity >= similarity_threshold:
                    similar_tasks.append(task2)
                    similar_indices.append(j)
            
            # –û–±—ä–µ–¥–∏–Ω–∏—Ç—å –ø–æ—Ö–æ–∂–∏–µ –∑–∞–¥–∞—á–∏
            if len(similar_tasks) > 1:
                # –í—ã–±—Ä–∞—Ç—å –∑–∞–¥–∞—á—É —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º completion_score –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω—É—é
                main_task = max(similar_tasks, key=lambda t: t.get('completion', {}).get('completion_score', 0))
                
                # –û–±—ä–µ–¥–∏–Ω–∏—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–∏
                all_sources = list(set(t.get('plan_source', 'unknown') for t in similar_tasks))
                main_task['plan_source'] = ', '.join(all_sources)
                main_task['merged_from'] = [t.get('id') for t in similar_tasks if t.get('id') != main_task.get('id')]
                main_task['is_merged'] = True
                main_task['merge_count'] = len(similar_tasks)
                
                task_groups.append(main_task)
                processed.update(similar_indices)
            else:
                task_groups.append(task1)
                processed.add(i)
        
        logger.info(f"–î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è: {len(tasks)} -> {len(task_groups)} –∑–∞–¥–∞—á")
        return task_groups
    
    def categorize_task(self, task_text: str) -> Dict[str, str]:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∏ —Å–ª–æ–∂–Ω–æ—Å—Ç—å –∑–∞–¥–∞—á–∏"""
        task_lower = task_text.lower()
        
        # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø –∑–∞–¥–∞—á–∏
        task_type = "feature"
        if any(word in task_lower for word in ['–∏—Å–ø—Ä–∞–≤–∏—Ç—å', 'fix', 'bug', '–æ—à–∏–±–∫–∞', 'error']):
            task_type = "bugfix"
        elif any(word in task_lower for word in ['—Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥', 'refactor', '—É–ª—É—á—à–∏—Ç—å', 'improve', '–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å']):
            task_type = "refactoring"
        elif any(word in task_lower for word in ['–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è', 'documentation', 'docs', 'guide', 'readme']):
            task_type = "documentation"
        elif any(word in task_lower for word in ['–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ', 'research', '—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç', 'experiment', '–ø—Ä–æ—Ç–æ—Ç–∏–ø']):
            task_type = "research"
        
        # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Å–ª–æ–∂–Ω–æ—Å—Ç—å
        complexity = "medium"
        simple_keywords = ['–¥–æ–±–∞–≤–∏—Ç—å', '—Å–æ–∑–¥–∞—Ç—å', '–¥–æ–±–∞–≤—å', 'create', 'add', 'simple', '–ø—Ä–æ—Å—Ç–æ–π']
        complex_keywords = ['—Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å', '—Ä–∞–∑—Ä–∞–±–æ—Ç–∞—Ç—å', '–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞', '—Å–∏—Å—Ç–µ–º–∞', '–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è', 'implement', 'develop', 'architecture', 'system']
        
        simple_count = sum(1 for kw in simple_keywords if kw in task_lower)
        complex_count = sum(1 for kw in complex_keywords if kw in task_lower)
        
        if simple_count > complex_count and len(task_text) < 100:
            complexity = "simple"
        elif complex_count > simple_count or len(task_text) > 200:
            complexity = "complex"
        
        # –û—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ (–≤ —á–∞—Å–∞—Ö)
        estimated_hours = 4  # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        if complexity == "simple":
            estimated_hours = 1
        elif complexity == "medium":
            estimated_hours = 4
        else:
            estimated_hours = 8
        
        return {
            "type": task_type,
            "complexity": complexity,
            "estimated_hours": estimated_hours
        }
    
    def parse_tasks_from_markdown(self, content: str, plan_name: str) -> List[Dict]:
        """–ü–∞—Ä—Å–∏—Ç—å –∑–∞–¥–∞—á–∏ –∏–∑ markdown"""
        tasks = []
        lines = content.split('\n')
        
        current_section = None
        current_subsection = None
        task_counter = 0
        
        for i, line in enumerate(lines):
            # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Å–µ–∫—Ü–∏—é (##)
            if line.startswith('##') and not line.startswith('###'):
                current_section = line.strip('#').strip()
                current_subsection = None
            # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø–æ–¥—Å–µ–∫—Ü–∏—é (###)
            elif line.startswith('###'):
                current_subsection = line.strip('#').strip()
            
            # –ù–∞–π—Ç–∏ –∑–∞–¥–∞—á–∏ —Å —á–µ–∫–±–æ–∫—Å–∞–º–∏ [x] –∏–ª–∏ [ ]
            checkbox_match = re.match(r'^[-*]\s*\[([ xX])\]\s*(.+)', line)
            if checkbox_match:
                is_done = checkbox_match.group(1).lower() == 'x'
                task_text = checkbox_match.group(2).strip()
                task_counter += 1
                
                # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∑–∞–¥–∞—á—É
                category_info = self.categorize_task(task_text)
                
                tasks.append({
                    "id": f"{plan_name}_task_{task_counter}",
                    "section": current_section,
                    "subsection": current_subsection,
                    "text": task_text,
                    "status": "done" if is_done else "todo",
                    "line_number": i + 1,
                    "plan_source": plan_name,
                    "task_type": category_info["type"],
                    "complexity": category_info["complexity"],
                    "estimated_hours": category_info["estimated_hours"]
                })
            
            # –ù–∞–π—Ç–∏ –∑–∞–¥–∞—á–∏ –±–µ–∑ —á–µ–∫–±–æ–∫—Å–æ–≤ (–ø—Ä–æ–Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏ –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ –ø—É–Ω–∫—Ç—ã)
            elif re.match(r'^\d+\.\s+(.+)', line) or re.match(r'^[-*]\s+(.+)', line):
                # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ —ç—Ç–æ –Ω–µ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ –Ω–µ –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞
                task_text = re.sub(r'^\d+\.\s+', '', line).strip()
                task_text = re.sub(r'^[-*]\s+', '', task_text).strip()
                
                if task_text and not task_text.startswith('#') and len(task_text) > 10:
                    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–π –∑–∞–¥–∞—á–µ–π
                    if not any(t['text'] == task_text for t in tasks):
                        task_counter += 1
                        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∑–∞–¥–∞—á—É
                        category_info = self.categorize_task(task_text)
                        
                        tasks.append({
                            "id": f"{plan_name}_task_{task_counter}",
                            "section": current_section,
                            "subsection": current_subsection,
                            "text": task_text,
                            "status": "unknown",
                            "line_number": i + 1,
                            "plan_source": plan_name,
                            "task_type": category_info["type"],
                            "complexity": category_info["complexity"],
                            "estimated_hours": category_info["estimated_hours"]
                        })
        
        return tasks
    
    def check_file_exists(self, file_path: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞"""
        # –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –ø—É—Ç–µ–π
        paths_to_check = [
            self.backend_dir / file_path,
            self.project_root / file_path,
            self.backend_dir / "app" / file_path,
        ]
        
        for path in paths_to_check:
            if path.exists():
                return True
        return False
    
    def check_class_exists(self, class_name: str, file_path: Optional[str] = None) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∞ –∏ –µ–≥–æ –∫–ª—é—á–µ–≤—ã—Ö –º–µ—Ç–æ–¥–æ–≤"""
        result = {
            "exists": False,
            "file": None,
            "methods": [],
            "has_key_methods": False
        }
        
        if file_path:
            full_path = self.backend_dir / file_path
            if full_path.exists():
                try:
                    content = full_path.read_text(encoding="utf-8")
                    pattern = rf'class\s+{re.escape(class_name)}\s*[\(:]'
                    if re.search(pattern, content):
                        result["exists"] = True
                        result["file"] = str(full_path.relative_to(self.backend_dir))
                        # –ù–∞–π—Ç–∏ –º–µ—Ç–æ–¥—ã –∫–ª–∞—Å—Å–∞
                        class_start = content.find(f"class {class_name}")
                        if class_start != -1:
                            # –ù–∞–π—Ç–∏ –∫–æ–Ω–µ—Ü –∫–ª–∞—Å—Å–∞ (—Å–ª–µ–¥—É—é—â–∏–π –∫–ª–∞—Å—Å –∏–ª–∏ –∫–æ–Ω–µ—Ü —Ñ–∞–π–ª–∞)
                            next_class = content.find("\nclass ", class_start + 1)
                            class_content = content[class_start:next_class] if next_class != -1 else content[class_start:]
                            # –ù–∞–π—Ç–∏ –≤—Å–µ –º–µ—Ç–æ–¥—ã
                            method_pattern = r'def\s+(\w+)\s*\('
                            methods = re.findall(method_pattern, class_content)
                            result["methods"] = methods
                            # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –º–µ—Ç–æ–¥–æ–≤ (async def, def —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º–∏ –∏–º–µ–Ω–∞–º–∏)
                            key_methods = ['generate', 'execute', 'create', 'update', 'delete', 'get', 'save', 'load']
                            result["has_key_methods"] = any(m in methods for m in key_methods)
                        return result
                except:
                    pass
        
        # –ü–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º—É –ø—Ä–æ–µ–∫—Ç—É
        for py_file in self.backend_dir.rglob("*.py"):
            try:
                content = py_file.read_text(encoding="utf-8")
                pattern = rf'class\s+{re.escape(class_name)}\s*[\(:]'
                if re.search(pattern, content):
                    result["exists"] = True
                    result["file"] = str(py_file.relative_to(self.backend_dir))
                    # –ù–∞–π—Ç–∏ –º–µ—Ç–æ–¥—ã –∫–ª–∞—Å—Å–∞
                    class_start = content.find(f"class {class_name}")
                    if class_start != -1:
                        next_class = content.find("\nclass ", class_start + 1)
                        class_content = content[class_start:next_class] if next_class != -1 else content[class_start:]
                        method_pattern = r'def\s+(\w+)\s*\('
                        methods = re.findall(method_pattern, class_content)
                        result["methods"] = methods
                        key_methods = ['generate', 'execute', 'create', 'update', 'delete', 'get', 'save', 'load']
                        result["has_key_methods"] = any(m in methods for m in key_methods)
                    return result
            except:
                continue
        
        return result
    
    def check_migration_exists(self, migration_name: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –º–∏–≥—Ä–∞—Ü–∏–∏"""
        migrations_dir = self.backend_dir / "alembic" / "versions"
        if not migrations_dir.exists():
            return False
        
        # –ò—Å–∫–∞—Ç—å –ø–æ —á–∞—Å—Ç–∏ –∏–º–µ–Ω–∏ –∏–ª–∏ –Ω–æ–º–µ—Ä—É —Ä–µ–≤–∏–∑–∏–∏
        migration_name_lower = migration_name.lower()
        for migration_file in migrations_dir.glob("*.py"):
            file_name_lower = migration_file.name.lower()
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –∏–º–µ–Ω–∏ –∏–ª–∏ –Ω–æ–º–µ—Ä—É —Ä–µ–≤–∏–∑–∏–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "017" –∏–ª–∏ "017_extend")
            if migration_name_lower in file_name_lower:
                return True
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –Ω–æ–º–µ—Ä—É —Ä–µ–≤–∏–∑–∏–∏ –≤ –Ω–∞—á–∞–ª–µ —Ñ–∞–π–ª–∞
            try:
                content = migration_file.read_text(encoding="utf-8")
                # –ò—Å–∫–∞—Ç—å revision ID –≤ —Ñ–∞–π–ª–µ
                revision_match = re.search(r'revision\s*:\s*str\s*=\s*[\'\"](\d+)[\'\"]', content)
                if revision_match:
                    revision_id = revision_match.group(1)
                    if revision_id in migration_name or migration_name in revision_id:
                        return True
            except:
                continue
        
        return False
    
    def check_api_endpoint_exists(self, endpoint_path: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞–ª–∏—á–∏–µ API endpoint"""
        # –ü–æ–∏—Å–∫ –≤ routes —Ñ–∞–π–ª–∞—Ö
        routes_dir = self.backend_dir / "app" / "api" / "routes"
        if not routes_dir.exists():
            return False
        
        for route_file in routes_dir.glob("*.py"):
            try:
                content = route_file.read_text(encoding="utf-8")
                # –ò—Å–∫–∞—Ç—å –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä—ã @router.get, @router.post –∏ —Ç.–¥.
                endpoint_pattern = rf'@router\.(get|post|put|delete|patch)\s*\(["\']([^"\']+)["\']'
                matches = re.findall(endpoint_pattern, content)
                for method, path in matches:
                    if endpoint_path in path or path in endpoint_path:
                        return True
            except:
                continue
        
        return False
    
    def check_template_exists(self, template_path: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞–ª–∏—á–∏–µ —à–∞–±–ª–æ–Ω–∞"""
        templates_dir = self.project_root / "frontend" / "templates"
        if not templates_dir.exists():
            return False
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø—É—Ç—å
        template_path = template_path.replace("\\\\", "/")
        if template_path.startswith("frontend/templates/"):
            template_path = template_path.replace("frontend/templates/", "")
        
        template_file = templates_dir / template_path
        return template_file.exists()
    
    def check_router_in_main(self, router_name: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞–ª–∏—á–∏–µ —Ä–æ—É—Ç–µ—Ä–∞ –≤ main.py"""
        main_file = self.backend_dir / "main.py"
        if not main_file.exists():
            return False
        
        try:
            content = main_file.read_text(encoding="utf-8")
            # –ò—Å–∫–∞—Ç—å –∏–º–ø–æ—Ä—Ç –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–æ—É—Ç–µ—Ä–∞
            patterns = [
                rf'from\s+app\.api\.routes\.{re.escape(router_name)}\s+import',
                rf'import\s+{re.escape(router_name)}',
                rf'app\.include_router.*{re.escape(router_name)}'
            ]
            for pattern in patterns:
                if re.search(pattern, content, re.IGNORECASE):
                    return True
        except:
            pass
        
        return False
    
    def check_test_exists(self, component_name: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞–ª–∏—á–∏–µ —Ç–µ—Å—Ç–æ–≤ –¥–ª—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞"""
        tests_dir = self.backend_dir / "tests"
        if not tests_dir.exists():
            return False
        
        # –ü–æ–∏—Å–∫ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤
        test_patterns = [
            f"test_{component_name.lower()}",
            f"test_{component_name.lower().replace('_service', '')}",
            component_name.lower()
        ]
        
        for test_file in tests_dir.rglob("test_*.py"):
            file_name_lower = test_file.stem.lower()
            for pattern in test_patterns:
                if pattern in file_name_lower:
                    return True
        
        return False
    
    def check_documentation_exists(self, component_name: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞–ª–∏—á–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏"""
        docs_dir = self.project_root / "docs"
        if not docs_dir.exists():
            return False
        
        # –ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
        component_lower = component_name.lower()
        for doc_file in docs_dir.rglob("*.md"):
            file_name_lower = doc_file.stem.lower()
            if component_lower in file_name_lower or component_lower.replace("_", " ") in file_name_lower:
                return True
        
        return False
    
    def analyze_task_completion(self, task: Dict) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–æ–π"""
        task_text = task['text']
        completion_info = {
            "files_mentioned": [],
            "classes_mentioned": [],
            "migrations_mentioned": [],
            "endpoints_mentioned": [],
            "templates_mentioned": [],
            "files_exist": {},
            "classes_exist": {},
            "migrations_exist": {},
            "endpoints_exist": {},
            "templates_exist": {},
            "routers_in_main": {},
            "tests_exist": {},
            "docs_exist": {},
            "completion_score": 0.0,
            "completion_details": []
        }
        
        # –ù–∞–π—Ç–∏ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è —Ñ–∞–π–ª–æ–≤
        file_patterns = re.findall(r'`([^`]+\.py)`', task_text)
        file_patterns.extend(re.findall(r'([a-z_/]+\.py)', task_text, re.IGNORECASE))
        
        for file_pattern in file_patterns:
            if file_pattern not in completion_info["files_mentioned"]:
                completion_info["files_mentioned"].append(file_pattern)
                exists = self.check_file_exists(file_pattern)
                completion_info["files_exist"][file_pattern] = exists
                if exists:
                    completion_info["completion_score"] += 0.25
                    completion_info["completion_details"].append(f"–§–∞–π–ª {file_pattern} —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
                    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞–ª–∏—á–∏–µ —Ç–µ—Å—Ç–æ–≤ –¥–ª—è —ç—Ç–æ–≥–æ —Ñ–∞–π–ª–∞
                    component_name = Path(file_pattern).stem
                    has_tests = self.check_test_exists(component_name)
                    completion_info["tests_exist"][component_name] = has_tests
                    if has_tests:
                        completion_info["completion_score"] += 0.1
                        completion_info["completion_details"].append(f"–¢–µ—Å—Ç—ã –¥–ª—è {component_name} –Ω–∞–π–¥–µ–Ω—ã")
        
        # –ù–∞–π—Ç–∏ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤
        class_patterns = re.findall(r'–∫–ª–∞—Å—Å\s+(\w+)|class\s+(\w+)', task_text, re.IGNORECASE)
        for match in class_patterns:
            class_name = match[0] or match[1]
            if class_name and class_name not in completion_info["classes_mentioned"]:
                completion_info["classes_mentioned"].append(class_name)
                class_info = self.check_class_exists(class_name)
                completion_info["classes_exist"][class_name] = class_info
                if class_info["exists"]:
                    completion_info["completion_score"] += 0.15
                    completion_info["completion_details"].append(f"–ö–ª–∞—Å—Å {class_name} —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
                    if class_info["has_key_methods"]:
                        completion_info["completion_score"] += 0.1
                        completion_info["completion_details"].append(f"–ö–ª–∞—Å—Å {class_name} –∏–º–µ–µ—Ç –∫–ª—é—á–µ–≤—ã–µ –º–µ—Ç–æ–¥—ã")
        
        # –ù–∞–π—Ç–∏ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –º–∏–≥—Ä–∞—Ü–∏–π
        migration_patterns = re.findall(r'–º–∏–≥—Ä–∞—Ü–∏[—è–∏]\s+(\d+[a-z_]+)', task_text, re.IGNORECASE)
        migration_patterns.extend(re.findall(r'migration\s+(\d+[a-z_]+)', task_text, re.IGNORECASE))
        migration_patterns.extend(re.findall(r'(\d{3}_[a-z_]+)', task_text, re.IGNORECASE))  # 017_extend_task_lifecycle
        
        for migration_pattern in migration_patterns:
            if migration_pattern not in completion_info["migrations_mentioned"]:
                completion_info["migrations_mentioned"].append(migration_pattern)
                exists = self.check_migration_exists(migration_pattern)
                completion_info["migrations_exist"][migration_pattern] = exists
                if exists:
                    completion_info["completion_score"] += 0.15
                    completion_info["completion_details"].append(f"–ú–∏–≥—Ä–∞—Ü–∏—è {migration_pattern} –Ω–∞–π–¥–µ–Ω–∞")
        
        # –ù–∞–π—Ç–∏ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è API endpoints
        endpoint_patterns = re.findall(r'/(api/[^"\'\s]+)', task_text, re.IGNORECASE)
        endpoint_patterns.extend(re.findall(r'endpoint[:\s]+([/a-z_]+)', task_text, re.IGNORECASE))
        
        for endpoint in endpoint_patterns:
            if endpoint not in completion_info["endpoints_mentioned"]:
                completion_info["endpoints_mentioned"].append(endpoint)
                exists = self.check_api_endpoint_exists(endpoint)
                completion_info["endpoints_exist"][endpoint] = exists
                if exists:
                    completion_info["completion_score"] += 0.1
                    completion_info["completion_details"].append(f"API endpoint {endpoint} –Ω–∞–π–¥–µ–Ω")
        
        # –ù–∞–π—Ç–∏ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è —à–∞–±–ª–æ–Ω–æ–≤
        template_patterns = re.findall(r'`([^`]+\.html)`', task_text)
        template_patterns.extend(re.findall(r'(frontend/templates/[^"\'\s]+\.html)', task_text, re.IGNORECASE))
        
        for template in template_patterns:
            if template not in completion_info["templates_mentioned"]:
                completion_info["templates_mentioned"].append(template)
                exists = self.check_template_exists(template)
                completion_info["templates_exist"][template] = exists
                if exists:
                    completion_info["completion_score"] += 0.1
                    completion_info["completion_details"].append(f"–®–∞–±–ª–æ–Ω {template} –Ω–∞–π–¥–µ–Ω")
        
        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é —Ä–æ—É—Ç–µ—Ä–æ–≤ –≤ main.py
        router_patterns = re.findall(r'router[:\s]+([a-z_]+)', task_text, re.IGNORECASE)
        router_patterns.extend(re.findall(r'([a-z_]+_pages?|plans|approvals|artifacts)', task_text, re.IGNORECASE))
        
        for router_name in router_patterns:
            if router_name not in completion_info["routers_in_main"]:
                exists = self.check_router_in_main(router_name)
                completion_info["routers_in_main"][router_name] = exists
                if exists:
                    completion_info["completion_score"] += 0.1
                    completion_info["completion_details"].append(f"–†–æ—É—Ç–µ—Ä {router_name} –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω –≤ main.py")
        
        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é
        doc_keywords = re.findall(r'–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏[—è–∏]|docs?/|guides?/', task_text, re.IGNORECASE)
        if doc_keywords:
            # –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –Ω–∞–π—Ç–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
            for class_name in completion_info["classes_mentioned"]:
                has_docs = self.check_documentation_exists(class_name)
                completion_info["docs_exist"][class_name] = has_docs
                if has_docs:
                    completion_info["completion_score"] += 0.05
                    completion_info["completion_details"].append(f"–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –¥–ª—è {class_name} –Ω–∞–π–¥–µ–Ω–∞")
        
        # –ï—Å–ª–∏ –∑–∞–¥–∞—á–∞ –ø–æ–º–µ—á–µ–Ω–∞ –∫–∞–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω–∞—è –≤ –ø–ª–∞–Ω–µ
        if task.get('status') == 'done':
            completion_info["completion_score"] = max(completion_info["completion_score"], 0.8)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å score
        completion_info["completion_score"] = min(completion_info["completion_score"], 1.0)
        
        return completion_info
    
    async def analyze_with_llm(self, all_tasks: List[Dict], all_plans: List[Dict]) -> Dict[str, Any]:
        """–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å LLM –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Å–≤—è–∑–µ–π –∏ –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–∏"""
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ - —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ª–∏–º–∏—Ç –¥–æ 500
        tasks_summary = []
        for task in all_tasks[:500]:  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 100 –¥–æ 500
            completion = task.get("completion", {})
            tasks_summary.append({
                "id": task.get("id", ""),
                "text": task.get("text", "")[:300],  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 200 –¥–æ 300
                "section": task.get("section", ""),
                "status": task.get("status", ""),
                "completion_score": completion.get("completion_score", 0.0),
                "files_exist": sum(1 for v in completion.get("files_exist", {}).values() if v),
                "classes_exist": sum(1 for v in completion.get("classes_exist", {}).values() if isinstance(v, dict) and v.get("exists")),
                "has_tests": any(completion.get("tests_exist", {}).values()),
                "has_docs": any(completion.get("docs_exist", {}).values())
            })
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞—Ö
        implemented_components = {
            "services": [],
            "models": [],
            "routes": [],
            "templates": []
        }
        
        # –°–æ–±—Ä–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞—Ö
        services_dir = self.backend_dir / "app" / "services"
        if services_dir.exists():
            for service_file in services_dir.glob("*.py"):
                if service_file.stem != "__init__":
                    implemented_components["services"].append(service_file.stem)
        
        models_dir = self.backend_dir / "app" / "models"
        if models_dir.exists():
            for model_file in models_dir.glob("*.py"):
                if model_file.stem != "__init__":
                    implemented_components["models"].append(model_file.stem)
        
        routes_dir = self.backend_dir / "app" / "api" / "routes"
        if routes_dir.exists():
            for route_file in routes_dir.glob("*.py"):
                if route_file.stem != "__init__":
                    implemented_components["routes"].append(route_file.stem)
        
        plans_summary = []
        for plan in all_plans[:30]:  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 20 –¥–æ 30
            plans_summary.append({
                "name": plan.get("name", ""),
                "source": plan.get("source", ""),
                "size": plan.get("size", 0)
            })
        
        # –ú–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω—ã–π –∞–Ω–∞–ª–∏–∑: –≠—Ç–∞–ø 1 - –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è
        prompt_stage1 = f"""–¢—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—à—å –ø–ª–∞–Ω—ã —Ä–∞–∑–≤–∏—Ç–∏—è –ø—Ä–æ–µ–∫—Ç–∞ AARD (–∞–≤—Ç–æ–Ω–æ–º–Ω–∞—è –∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞).

–†–ï–ê–õ–ò–ó–û–í–ê–ù–ù–´–ï –ö–û–ú–ü–û–ù–ï–ù–¢–´:
–°–µ—Ä–≤–∏—Å—ã: {', '.join(implemented_components['services'][:20])}
–ú–æ–¥–µ–ª–∏: {', '.join(implemented_components['models'][:20])}
API Routes: {', '.join(implemented_components['routes'][:20])}

–°–¢–ê–¢–ò–°–¢–ò–ö–ê:
- –í—Å–µ–≥–æ –ø–ª–∞–Ω–æ–≤: {len(all_plans)}
- –í—Å–µ–≥–æ –∑–∞–¥–∞—á: {len(all_tasks)}
- –í—ã–ø–æ–ª–Ω–µ–Ω–æ –∑–∞–¥–∞—á: {sum(1 for t in all_tasks if t.get('completion', {}).get('completion_score', 0) >= 0.8)}
- –í –ø—Ä–æ—Ü–µ—Å—Å–µ: {sum(1 for t in all_tasks if 0.3 <= t.get('completion', {}).get('completion_score', 0) < 0.8)}
- –ù–µ –Ω–∞—á–∞—Ç–æ: {sum(1 for t in all_tasks if t.get('completion', {}).get('completion_score', 0) < 0.3)}

–ó–ê–î–ê–ß–ò –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê:
{json.dumps(tasks_summary[:200], indent=2, ensure_ascii=False)}

–≠–¢–ê–ü 1: –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è
–°–≥—Ä—É–ø–ø–∏—Ä—É–π –∑–∞–¥–∞—á–∏ –ø–æ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º –æ–±–ª–∞—Å—Ç—è–º –∏ –æ—Ç–≤–µ—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
{{
  "functional_areas": [
    {{
      "name": "–Ω–∞–∑–≤–∞–Ω–∏–µ –æ–±–ª–∞—Å—Ç–∏ (Core Infrastructure, Planning System, Execution Engine, Human-in-the-Loop, Learning & Improvement, Security & Safety, UI & UX, Agent System, Observability, Testing & Quality)",
      "description": "–æ–ø–∏—Å–∞–Ω–∏–µ –æ–±–ª–∞—Å—Ç–∏",
      "task_ids": ["id1", "id2"],
      "priority": "high/medium/low",
      "completion_rate": 0.0-1.0
    }}
  ],
  "task_categories": [
    {{
      "task_id": "id",
      "category": "feature/bugfix/refactoring/documentation/research",
      "complexity": "simple/medium/complex",
      "estimated_hours": —á–∏—Å–ª–æ
    }}
  ]
}}

–û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û JSON, –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞."""

        try:
            # –ü–æ–ª—É—á–∏—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ Ollama –∏–Ω—Å—Ç–∞–Ω—Å–∞–º
            settings = get_settings()
            instance = settings.ollama_instance_1
            server_url = instance.url
            model = instance.model
            
            # –≠–¢–ê–ü 1: –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è
            logger.info("LLM –∞–Ω–∞–ª–∏–∑: –≠—Ç–∞–ø 1 - –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è")
            response1 = await self.ollama_client.generate(
                prompt=prompt_stage1,
                task_type=TaskType.REASONING,
                model=model,
                server_url=server_url,
                temperature=0.3
            )
            
            response_text1 = response1.response.strip()
            if response_text1.startswith("```"):
                response_text1 = re.sub(r'^```(?:json)?\s*\n', '', response_text1)
                response_text1 = re.sub(r'\n```\s*$', '', response_text1)
            
            stage1_result = json.loads(response_text1)
            
            # –≠–¢–ê–ü 2: –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
            logger.info("LLM –∞–Ω–∞–ª–∏–∑: –≠—Ç–∞–ø 2 - –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π")
            prompt_stage2 = f"""–≠–¢–ê–ü 2: –ê–Ω–∞–ª–∏–∑ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –∏ –±–ª–æ–∫–µ—Ä–æ–≤

–†–ï–ó–£–õ–¨–¢–ê–¢–´ –≠–¢–ê–ü–ê 1:
{json.dumps(stage1_result, indent=2, ensure_ascii=False)}

–ó–ê–î–ê–ß–ò:
{json.dumps(tasks_summary[:300], indent=2, ensure_ascii=False)}

–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –º–µ–∂–¥—É –∑–∞–¥–∞—á–∞–º–∏ –∏ –≤—ã—è–≤–∏ –±–ª–æ–∫–µ—Ä—ã. –û—Ç–≤–µ—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
{{
  "dependencies": [
    {{
      "task_id": "id –∑–∞–¥–∞—á–∏",
      "depends_on": ["id –∑–∞–≤–∏—Å–∏–º—ã—Ö –∑–∞–¥–∞—á"],
      "reason": "—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –ø—Ä–∏—á–∏–Ω–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏",
      "is_blocker": true/false
    }}
  ],
  "blockers": [
    {{
      "task_id": "id –±–ª–æ–∫–µ—Ä–∞",
      "blocks": ["id –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–¥–∞—á"],
      "priority": "critical/high/medium"
    }}
  ],
  "critical_path": [
    "id –∑–∞–¥–∞—á–∏ –≤ –ø–æ—Ä—è–¥–∫–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—É—Ç–∏"
  ],
  "technical_debt": [
    {{
      "task_id": "id",
      "debt_type": "code_quality/architecture/testing/documentation",
      "severity": "high/medium/low",
      "description": "–æ–ø–∏—Å–∞–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –¥–æ–ª–≥–∞"
    }}
  ]
}}

–û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û JSON, –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞."""
            
            response2 = await self.ollama_client.generate(
                prompt=prompt_stage2,
                task_type=TaskType.REASONING,
                model=model,
                server_url=server_url,
                temperature=0.3
            )
            
            response_text2 = response2.response.strip()
            if response_text2.startswith("```"):
                response_text2 = re.sub(r'^```(?:json)?\s*\n', '', response_text2)
                response_text2 = re.sub(r'\n```\s*$', '', response_text2)
            
            stage2_result = json.loads(response_text2)
            
            # –≠–¢–ê–ü 3: –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            logger.info("LLM –∞–Ω–∞–ª–∏–∑: –≠—Ç–∞–ø 3 - –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏")
            prompt_stage3 = f"""–≠–¢–ê–ü 3: –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

–†–ï–ó–£–õ–¨–¢–ê–¢–´ –≠–¢–ê–ü–ê 1:
{json.dumps(stage1_result, indent=2, ensure_ascii=False)}

–†–ï–ó–£–õ–¨–¢–ê–¢–´ –≠–¢–ê–ü–ê 2:
{json.dumps(stage2_result, indent=2, ensure_ascii=False)}

–†–ï–ê–õ–ò–ó–û–í–ê–ù–ù–´–ï –ö–û–ú–ü–û–ù–ï–ù–¢–´:
{json.dumps(implemented_components, indent=2, ensure_ascii=False)}

–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∏ —Å–æ–∑–¥–∞–π –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π roadmap. –û—Ç–≤–µ—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
{{
  "analysis": {{
    "total_plans": {len(all_plans)},
    "total_tasks": {len(all_tasks)},
    "completed_tasks": {sum(1 for t in all_tasks if t.get('completion', {}).get('completion_score', 0) >= 0.8)},
    "in_progress_tasks": {sum(1 for t in all_tasks if 0.3 <= t.get('completion', {}).get('completion_score', 0) < 0.8)},
    "todo_tasks": {sum(1 for t in all_tasks if t.get('completion', {}).get('completion_score', 0) < 0.3)}
  }},
  "roadmap": {{
    "immediate_goals": [
      {{
        "task_id": "id",
        "goal": "–æ–ø–∏—Å–∞–Ω–∏–µ —Ü–µ–ª–∏",
        "timeframe": "1-2 –Ω–µ–¥–µ–ª–∏",
        "priority": "critical/high"
      }}
    ],
    "short_term_goals": [
      {{
        "task_id": "id",
        "goal": "–æ–ø–∏—Å–∞–Ω–∏–µ —Ü–µ–ª–∏",
        "timeframe": "1-3 –º–µ—Å—è—Ü–∞",
        "priority": "high/medium"
      }}
    ],
    "long_term_goals": [
      {{
        "task_id": "id",
        "goal": "–æ–ø–∏—Å–∞–Ω–∏–µ —Ü–µ–ª–∏",
        "timeframe": "3+ –º–µ—Å—è—Ü–∞",
        "priority": "medium/low"
      }}
    ]
  }},
  "recommendations": [
    {{
      "type": "architecture/priority/technical_debt/process",
      "priority": "high/medium/low",
      "description": "—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è",
      "impact": "–æ–ø–∏—Å–∞–Ω–∏–µ –≤–ª–∏—è–Ω–∏—è"
    }}
  ],
  "priority_order": [
    "id –∑–∞–¥–∞—á–∏ –≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è"
  ],
  "themes": [
    {{
      "name": "–Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–µ–º—ã",
      "description": "–æ–ø–∏—Å–∞–Ω–∏–µ",
      "tasks_count": —á–∏—Å–ª–æ,
      "priority": "high/medium/low",
      "completion_rate": 0.0-1.0
    }}
  ]
}}

–û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û JSON, –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞."""
            
            response3 = await self.ollama_client.generate(
                prompt=prompt_stage3,
                task_type=TaskType.REASONING,
                model=model,
                server_url=server_url,
                temperature=0.3
            )
            
            response_text3 = response3.response.strip()
            if response_text3.startswith("```"):
                response_text3 = re.sub(r'^```(?:json)?\s*\n', '', response_text3)
                response_text3 = re.sub(r'\n```\s*$', '', response_text3)
            
            stage3_result = json.loads(response_text3)
            
            # –û–±—ä–µ–¥–∏–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—Å–µ—Ö —ç—Ç–∞–ø–æ–≤
            analysis = {
                **stage3_result,
                "functional_areas": stage1_result.get("functional_areas", []),
                "task_categories": stage1_result.get("task_categories", []),
                "dependencies": stage2_result.get("dependencies", []),
                "blockers": stage2_result.get("blockers", []),
                "critical_path": stage2_result.get("critical_path", []),
                "technical_debt": stage2_result.get("technical_debt", [])
            }
            
            return analysis
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —á–µ—Ä–µ–∑ LLM: {e}", exc_info=True)
            return {
                "analysis": {
                    "total_plans": len(all_plans),
                    "total_tasks": len(all_tasks),
                    "error": str(e)
                },
                "themes": [],
                "dependencies": [],
                "recommendations": [],
                "priority_order": [],
                "functional_areas": [],
                "task_categories": [],
                "blockers": [],
                "critical_path": [],
                "technical_debt": [],
                "roadmap": {}
            }
    
    def generate_consolidated_plan(self, all_tasks: List[Dict], all_plans: List[Dict], 
                                   llm_analysis: Dict) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –µ–¥–∏–Ω—ã–π –∫–æ–Ω—Å–æ–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–ª–∞–Ω —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π"""
        output = []
        
        # Executive Summary
        output.append("# –ï–¥–∏–Ω—ã–π –∫–æ–Ω—Å–æ–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–ª–∞–Ω —Ä–∞–∑–≤–∏—Ç–∏—è AARD\n")
        output.append(f"*–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n\n")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        completed = [t for t in all_tasks if t.get('completion', {}).get('completion_score', 0) >= 0.8]
        in_progress = [t for t in all_tasks if 0.3 <= t.get('completion', {}).get('completion_score', 0) < 0.8]
        todo = [t for t in all_tasks if t.get('completion', {}).get('completion_score', 0) < 0.3]
        
        total_tasks = len(all_tasks)
        completion_rate = len(completed) / total_tasks if total_tasks > 0 else 0
        
        output.append("## Executive Summary\n\n")
        output.append(f"**–í—Å–µ–≥–æ –ø–ª–∞–Ω–æ–≤ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ:** {len(all_plans)}\n")
        output.append(f"**–í—Å–µ–≥–æ –∑–∞–¥–∞—á –Ω–∞–π–¥–µ–Ω–æ:** {total_tasks}\n")
        output.append(f"**–û–±—â–∏–π –ø—Ä–æ–≥—Ä–µ—Å—Å:** {completion_rate:.1%}\n\n")
        
        # –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
        progress_bar_length = 50
        filled = int(completion_rate * progress_bar_length)
        progress_bar = "‚ñà" * filled + "‚ñë" * (progress_bar_length - filled)
        output.append(f"`{progress_bar}` {completion_rate:.1%}\n\n")
        
        output.append("### –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å—Ç–∞—Ç—É—Å–∞–º\n\n")
        output.append(f"- ‚úÖ **–í—ã–ø–æ–ª–Ω–µ–Ω–æ:** {len(completed)} –∑–∞–¥–∞—á ({len(completed)/total_tasks*100:.1f}%)\n")
        output.append(f"- ‚è≥ **–í –ø—Ä–æ—Ü–µ—Å—Å–µ:** {len(in_progress)} –∑–∞–¥–∞—á ({len(in_progress)/total_tasks*100:.1f}%)\n")
        output.append(f"- ‚ùå **–ù–µ –Ω–∞—á–∞—Ç–æ:** {len(todo)} –∑–∞–¥–∞—á ({len(todo)/total_tasks*100:.1f}%)\n\n")
        
        # –¢–µ–∫—É—â–∏–π —Å—Ç–∞—Ç—É—Å –ø—Ä–æ–µ–∫—Ç–∞
        output.append("## –¢–µ–∫—É—â–∏–π —Å—Ç–∞—Ç—É—Å –ø—Ä–æ–µ–∫—Ç–∞\n\n")
        
        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –∑–∞–¥–∞—á –ø–æ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º –æ–±–ª–∞—Å—Ç—è–º –∏–∑ LLM –∞–Ω–∞–ª–∏–∑–∞
        functional_areas = llm_analysis.get("functional_areas", [])
        if functional_areas:
            output.append("### –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –ø–æ –æ–±–ª–∞—Å—Ç—è–º\n\n")
            for area in functional_areas:
                area_tasks = [t for t in all_tasks if t.get("id") in area.get("task_ids", [])]
                area_completed = [t for t in area_tasks if t.get('completion', {}).get('completion_score', 0) >= 0.8]
                area_rate = len(area_completed) / len(area_tasks) if area_tasks else 0
                
                priority_emoji = {"high": "üî¥", "medium": "üü°", "low": "üü¢"}.get(area.get("priority", "medium"), "‚ö™")
                output.append(f"#### {priority_emoji} {area.get('name', 'Unknown')}\n")
                output.append(f"{area.get('description', '')}\n")
                output.append(f"- **–ó–∞–¥–∞—á:** {len(area_tasks)}\n")
                output.append(f"- **–í—ã–ø–æ–ª–Ω–µ–Ω–æ:** {len(area_completed)} ({area_rate:.1%})\n")
                output.append(f"- **–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** {area.get('priority', 'medium')}\n\n")
        
        # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏ –∏ –±–ª–æ–∫–µ—Ä—ã
        blockers = llm_analysis.get("blockers", [])
        if blockers:
            output.append("### –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏ –∏ –±–ª–æ–∫–µ—Ä—ã\n\n")
            for blocker in blockers[:10]:
                task_id = blocker.get("task_id")
                task = next((t for t in all_tasks if t.get("id") == task_id), None)
                if task:
                    priority_emoji = {"critical": "üî¥", "high": "üü†", "medium": "üü°"}.get(blocker.get("priority", "medium"), "‚ö™")
                    output.append(f"- {priority_emoji} **{task.get('text', '')[:150]}**\n")
                    output.append(f"  - –ë–ª–æ–∫–∏—Ä—É–µ—Ç: {len(blocker.get('blocks', []))} –∑–∞–¥–∞—á\n")
                    output.append(f"  - –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: {blocker.get('priority', 'medium')}\n\n")
        
        # Roadmap
        roadmap = llm_analysis.get("roadmap", {})
        if roadmap:
            output.append("## Roadmap\n\n")
            
            # –ë–ª–∏–∂–∞–π—à–∏–µ —Ü–µ–ª–∏ (1-2 –Ω–µ–¥–µ–ª–∏)
            immediate = roadmap.get("immediate_goals", [])
            if immediate:
                output.append("### –ë–ª–∏–∂–∞–π—à–∏–µ —Ü–µ–ª–∏ (1-2 –Ω–µ–¥–µ–ª–∏)\n\n")
                for goal in immediate[:10]:
                    task_id = goal.get("task_id")
                    task = next((t for t in all_tasks if t.get("id") == task_id), None)
                    if task:
                        priority_emoji = {"critical": "üî¥", "high": "üü†"}.get(goal.get("priority", "high"), "üü°")
                        output.append(f"- {priority_emoji} **{goal.get('goal', task.get('text', ''))[:150]}**\n")
                        output.append(f"  - –í—Ä–µ–º—è: {goal.get('timeframe', '1-2 –Ω–µ–¥–µ–ª–∏')}\n")
                        output.append(f"  - –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: {goal.get('priority', 'high')}\n\n")
            
            # –°—Ä–µ–¥–Ω–µ—Å—Ä–æ—á–Ω—ã–µ —Ü–µ–ª–∏ (1-3 –º–µ—Å—è—Ü–∞)
            short_term = roadmap.get("short_term_goals", [])
            if short_term:
                output.append("### –°—Ä–µ–¥–Ω–µ—Å—Ä–æ—á–Ω—ã–µ —Ü–µ–ª–∏ (1-3 –º–µ—Å—è—Ü–∞)\n\n")
                for goal in short_term[:10]:
                    task_id = goal.get("task_id")
                    task = next((t for t in all_tasks if t.get("id") == task_id), None)
                    if task:
                        priority_emoji = {"high": "üü†", "medium": "üü°"}.get(goal.get("priority", "medium"), "üü¢")
                        output.append(f"- {priority_emoji} **{goal.get('goal', task.get('text', ''))[:150]}**\n")
                        output.append(f"  - –í—Ä–µ–º—è: {goal.get('timeframe', '1-3 –º–µ—Å—è—Ü–∞')}\n")
                        output.append(f"  - –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: {goal.get('priority', 'medium')}\n\n")
            
            # –î–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ —Ü–µ–ª–∏ (3+ –º–µ—Å—è—Ü–∞)
            long_term = roadmap.get("long_term_goals", [])
            if long_term:
                output.append("### –î–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ —Ü–µ–ª–∏ (3+ –º–µ—Å—è—Ü–∞)\n\n")
                for goal in long_term[:10]:
                    task_id = goal.get("task_id")
                    task = next((t for t in all_tasks if t.get("id") == task_id), None)
                    if task:
                        priority_emoji = {"medium": "üü°", "low": "üü¢"}.get(goal.get("priority", "low"), "‚ö™")
                        output.append(f"- {priority_emoji} **{goal.get('goal', task.get('text', ''))[:150]}**\n")
                        output.append(f"  - –í—Ä–µ–º—è: {goal.get('timeframe', '3+ –º–µ—Å—è—Ü–∞')}\n")
                        output.append(f"  - –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: {goal.get('priority', 'low')}\n\n")
        
        # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –¥–æ–ª–≥
        technical_debt = llm_analysis.get("technical_debt", [])
        if technical_debt:
            output.append("## –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –¥–æ–ª–≥\n\n")
            for debt in technical_debt[:15]:
                task_id = debt.get("task_id")
                task = next((t for t in all_tasks if t.get("id") == task_id), None)
                if task:
                    severity_emoji = {"high": "üî¥", "medium": "üü°", "low": "üü¢"}.get(debt.get("severity", "medium"), "‚ö™")
                    output.append(f"- {severity_emoji} **{debt.get('description', task.get('text', ''))[:150]}**\n")
                    output.append(f"  - –¢–∏–ø: {debt.get('debt_type', 'unknown')}\n")
                    output.append(f"  - –°–µ—Ä—å–µ–∑–Ω–æ—Å—Ç—å: {debt.get('severity', 'medium')}\n\n")
        
        # –ì—Ä–∞—Ñ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π ASCII)
        dependencies = llm_analysis.get("dependencies", [])
        critical_path = llm_analysis.get("critical_path", [])
        if critical_path:
            output.append("## –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –ø—É—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è\n\n")
            output.append("```\n")
            task_map = {t.get("id"): t for t in all_tasks}
            for i, task_id in enumerate(critical_path[:15]):
                task = task_map.get(task_id)
                if task:
                    arrow = " -> " if i < len(critical_path) - 1 else ""
                    output.append(f"[{i+1}] {task.get('text', '')[:80]}{arrow}\n")
            output.append("```\n\n")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        recommendations = llm_analysis.get("recommendations", [])
        if recommendations:
            output.append("## –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\n\n")
            for i, rec in enumerate(recommendations, 1):
                rec_type = rec.get("type", "general") if isinstance(rec, dict) else "general"
                priority = rec.get("priority", "medium") if isinstance(rec, dict) else "medium"
                description = rec.get("description", rec) if isinstance(rec, dict) else rec
                impact = rec.get("impact", "") if isinstance(rec, dict) else ""
                
                priority_emoji = {"high": "üî¥", "medium": "üü°", "low": "üü¢"}.get(priority, "‚ö™")
                output.append(f"{i}. {priority_emoji} **{description}**\n")
                if isinstance(rec, dict):
                    output.append(f"   - –¢–∏–ø: {rec_type}\n")
                    if impact:
                        output.append(f"   - –í–ª–∏—è–Ω–∏–µ: {impact}\n")
                output.append("\n")
        
        # –í—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏
        if completed:
            output.append("## ‚úÖ –í—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏\n\n")
            for task in completed[:50]:  # –û–≥—Ä–∞–Ω–∏—á–∏—Ç—å –≤—ã–≤–æ–¥
                output.append(f"- ‚úÖ {task.get('text', '')[:100]}\n")
                output.append(f"  *–ò–∑ –ø–ª–∞–Ω–∞: {task.get('plan_source', 'unknown')}*\n")
            if len(completed) > 50:
                output.append(f"\n*... –∏ –µ—â–µ {len(completed) - 50} –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á*\n")
            output.append("\n")
        
        # –ó–∞–¥–∞—á–∏ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ
        if in_progress:
            output.append("## ‚è≥ –ó–∞–¥–∞—á–∏ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ\n\n")
            for task in in_progress[:30]:
                score = task.get('completion', {}).get('completion_score', 0)
                output.append(f"- ‚è≥ [{score:.0%}] {task.get('text', '')[:100]}\n")
                output.append(f"  *–ò–∑ –ø–ª–∞–Ω–∞: {task.get('plan_source', 'unknown')}*\n")
            if len(in_progress) > 30:
                output.append(f"\n*... –∏ –µ—â–µ {len(in_progress) - 30} –∑–∞–¥–∞—á –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ*\n")
            output.append("\n")
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –∑–∞–¥–∞—á–∏ (–∏–∑ LLM –∞–Ω–∞–ª–∏–∑–∞)
        if llm_analysis.get('priority_order'):
            output.append("## üéØ –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –∑–∞–¥–∞—á–∏ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è\n\n")
            priority_task_ids = llm_analysis['priority_order'][:20]
            task_map = {t.get('id'): t for t in all_tasks}
            
            for i, task_id in enumerate(priority_task_ids, 1):
                task = task_map.get(task_id)
                if task:
                    output.append(f"{i}. **{task.get('text', '')[:150]}**\n")
                    output.append(f"   - –ü–ª–∞–Ω: {task.get('plan_source', 'unknown')}\n")
                    if task.get('section'):
                        output.append(f"   - –°–µ–∫—Ü–∏—è: {task.get('section')}\n")
                    output.append("\n")
        
        # –î–µ—Ç–∞–ª—å–Ω—ã–µ –∑–∞–¥–∞—á–∏ –ø–æ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º –æ–±–ª–∞—Å—Ç—è–º
        output.append("## –î–µ—Ç–∞–ª—å–Ω—ã–µ –∑–∞–¥–∞—á–∏ –ø–æ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º –æ–±–ª–∞—Å—Ç—è–º\n\n")
        
        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –∑–∞–¥–∞—á –ø–æ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º –æ–±–ª–∞—Å—Ç—è–º
        if functional_areas:
            for area in functional_areas:
                area_name = area.get('name', 'Unknown')
                area_task_ids = area.get('task_ids', [])
                area_tasks = [t for t in all_tasks if t.get("id") in area_task_ids]
                
                if area_tasks:
                    priority_emoji = {"high": "üî¥", "medium": "üü°", "low": "üü¢"}.get(area.get("priority", "medium"), "‚ö™")
                    output.append(f"### {priority_emoji} {area_name}\n\n")
                    output.append(f"{area.get('description', '')}\n\n")
                    
                    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å –∑–∞–¥–∞—á–∏ –ø–æ —Å—Ç–∞—Ç—É—Å—É
                    area_completed = [t for t in area_tasks if t.get('completion', {}).get('completion_score', 0) >= 0.8]
                    area_in_progress = [t for t in area_tasks if 0.3 <= t.get('completion', {}).get('completion_score', 0) < 0.8]
                    area_todo = [t for t in area_tasks if t.get('completion', {}).get('completion_score', 0) < 0.3]
                    
                    if area_completed:
                        output.append(f"#### –í—ã–ø–æ–ª–Ω–µ–Ω–æ ({len(area_completed)})\n\n")
                        for task in area_completed[:10]:
                            output.append(f"- ‚úÖ {task.get('text', '')[:150]}\n")
                        if len(area_completed) > 10:
                            output.append(f"*... –∏ –µ—â–µ {len(area_completed) - 10} –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á*\n")
                        output.append("\n")
                    
                    if area_in_progress:
                        output.append(f"#### –í –ø—Ä–æ—Ü–µ—Å—Å–µ ({len(area_in_progress)})\n\n")
                        for task in area_in_progress[:10]:
                            score = task.get('completion', {}).get('completion_score', 0)
                            output.append(f"- ‚è≥ [{score:.0%}] {task.get('text', '')[:150]}\n")
                        if len(area_in_progress) > 10:
                            output.append(f"*... –∏ –µ—â–µ {len(area_in_progress) - 10} –∑–∞–¥–∞—á –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ*\n")
                        output.append("\n")
                    
                    if area_todo:
                        output.append(f"#### –ù–µ –Ω–∞—á–∞—Ç–æ ({len(area_todo)})\n\n")
                        for task in area_todo[:15]:
                            output.append(f"- ‚ùå {task.get('text', '')[:150]}\n")
                        if len(area_todo) > 15:
                            output.append(f"*... –∏ –µ—â–µ {len(area_todo) - 15} –Ω–µ –Ω–∞—á–∞—Ç—ã—Ö –∑–∞–¥–∞—á*\n")
                        output.append("\n")
        
        # –ò—Å—Ç–æ—Ä–∏—è –ø–ª–∞–Ω–æ–≤
        output.append("## –ò—Å—Ç–æ—Ä–∏—è –ø–ª–∞–Ω–æ–≤\n\n")
        for plan in all_plans[:30]:  # –û–≥—Ä–∞–Ω–∏—á–∏—Ç—å –≤—ã–≤–æ–¥
            source_emoji = "üìÅ" if plan['source'] == 'cursor_plans' else "üì¶" if plan['source'] == 'archive' else "üíæ"
            output.append(f"- {source_emoji} **{plan.get('name', 'Unknown')}** ({plan.get('source', 'unknown')})")
            if plan.get('modified'):
                output.append(f" - {plan['modified'][:10]}")
            output.append("\n")
        if len(all_plans) > 30:
            output.append(f"\n*... –∏ –µ—â–µ {len(all_plans) - 30} –ø–ª–∞–Ω–æ–≤*\n")
        output.append("\n")
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –∑–∞–¥–∞—á–∏ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        priority_order = llm_analysis.get('priority_order', [])
        if priority_order:
            output.append("## –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –∑–∞–¥–∞—á–∏ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è\n\n")
            task_map = {t.get('id'): t for t in all_tasks}
            for i, task_id in enumerate(priority_order[:30], 1):
                task = task_map.get(task_id)
                if task:
                    completion_score = task.get('completion', {}).get('completion_score', 0)
                    if completion_score < 0.8:  # –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –Ω–µ–∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–µ
                        status_emoji = "‚úÖ" if completion_score >= 0.8 else \
                                      "‚è≥" if completion_score >= 0.3 else "‚ùå"
                        output.append(f"{i}. {status_emoji} **{task.get('text', '')[:150]}**\n")
                        output.append(f"   - –ü—Ä–æ–≥—Ä–µ—Å—Å: {completion_score:.0%}\n")
                        output.append(f"   - –ü–ª–∞–Ω: {task.get('plan_source', 'unknown')}\n")
                        if task.get('section'):
                            output.append(f"   - –°–µ–∫—Ü–∏—è: {task.get('section')}\n")
                        output.append("\n")
        
        return "\n".join(output)
    
    def export_to_json(self, all_tasks: List[Dict], all_plans: List[Dict], 
                      llm_analysis: Dict) -> Dict[str, Any]:
        """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ JSON —Ñ–æ—Ä–º–∞—Ç"""
        return {
            "metadata": {
                "generated_at": datetime.now().isoformat(),
                "total_plans": len(all_plans),
                "total_tasks": len(all_tasks)
            },
            "plans": all_plans,
            "tasks": all_tasks,
            "analysis": llm_analysis,
            "statistics": {
                "completed": sum(1 for t in all_tasks if t.get('completion', {}).get('completion_score', 0) >= 0.8),
                "in_progress": sum(1 for t in all_tasks if 0.3 <= t.get('completion', {}).get('completion_score', 0) < 0.8),
                "todo": sum(1 for t in all_tasks if t.get('completion', {}).get('completion_score', 0) < 0.3)
            }
        }
    
    async def consolidate(self) -> tuple[str, Dict]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (–ø–ª–∞–Ω, llm_analysis)"""
        logger.info("–ù–∞—á–∞–ª–æ –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏ –ø–ª–∞–Ω–æ–≤...")
        
        # 1. –°–æ–±—Ä–∞—Ç—å –ø–ª–∞–Ω—ã –∏–∑ —Ñ–∞–π–ª–æ–≤
        logger.info("–°–±–æ—Ä –ø–ª–∞–Ω–æ–≤ –∏–∑ —Ñ–∞–π–ª–æ–≤...")
        file_plans = self.collect_file_plans()
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ –ø–ª–∞–Ω–æ–≤ –≤ —Ñ–∞–π–ª–∞—Ö: {len(file_plans)}")
        
        # 2. –°–æ–±—Ä–∞—Ç—å –ø–ª–∞–Ω—ã –∏–∑ –ë–î
        logger.info("–°–±–æ—Ä –ø–ª–∞–Ω–æ–≤ –∏–∑ –ë–î...")
        db_plans = self.collect_db_plans()
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ –ø–ª–∞–Ω–æ–≤ –≤ –ë–î: {len(db_plans)}")
        
        self.all_plans = file_plans + db_plans
        
        # 3. –ü–∞—Ä—Å–∏—Ç—å –∑–∞–¥–∞—á–∏ –∏–∑ —Ñ–∞–π–ª–æ–≤—ã—Ö –ø–ª–∞–Ω–æ–≤
        logger.info("–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–¥–∞—á –∏–∑ –ø–ª–∞–Ω–æ–≤...")
        for plan in file_plans:
            tasks = self.parse_tasks_from_markdown(plan['content'], plan['name'])
            self.all_tasks.extend(tasks)
        
        # 4. –ü–∞—Ä—Å–∏—Ç—å –∑–∞–¥–∞—á–∏ –∏–∑ –ø–ª–∞–Ω–æ–≤ –ë–î
        for plan in db_plans:
            if plan.get('steps'):
                for i, step in enumerate(plan.get('steps', []), 1):
                    if isinstance(step, dict):
                        step_text = step.get('description', '') or step.get('action', '') or str(step)
                    else:
                        step_text = str(step)
                    
                    if step_text:
                        category_info = self.categorize_task(step_text)
                        self.all_tasks.append({
                            "id": f"db_{plan.get('id', 'unknown')}_step_{i}",
                            "section": f"–ü–ª–∞–Ω: {plan.get('goal', 'Unknown')[:50]}",
                            "text": step_text,
                            "status": "unknown",
                            "plan_source": f"db_plan_{plan.get('id', 'unknown')[:8]}",
                            "task_type": category_info["type"],
                            "complexity": category_info["complexity"],
                            "estimated_hours": category_info["estimated_hours"]
                        })
        
        logger.info(f"–í—Å–µ–≥–æ –∑–∞–¥–∞—á –Ω–∞–π–¥–µ–Ω–æ: {len(self.all_tasks)}")
        
        # 5. –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –∑–∞–¥–∞—á
        logger.info("–î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –∑–∞–¥–∞—á...")
        self.all_tasks = self.deduplicate_tasks(self.all_tasks)
        logger.info(f"–ü–æ—Å–ª–µ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏: {len(self.all_tasks)} –∑–∞–¥–∞—á")
        
        # 6. –ê–Ω–∞–ª–∏–∑ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á
        logger.info("–ê–Ω–∞–ª–∏–∑ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á...")
        for task in self.all_tasks:
            completion = self.analyze_task_completion(task)
            task['completion'] = completion
        
        # 7. –ê–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ LLM
        logger.info("–ê–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ LLM...")
        llm_analysis = await self.analyze_with_llm(self.all_tasks, self.all_plans)
        
        # 8. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –µ–¥–∏–Ω–æ–≥–æ –ø–ª–∞–Ω–∞
        logger.info("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –µ–¥–∏–Ω–æ–≥–æ –ø–ª–∞–Ω–∞...")
        consolidated_plan = self.generate_consolidated_plan(self.all_tasks, self.all_plans, llm_analysis)
        
        return consolidated_plan, llm_analysis


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("=" * 70)
    print(" –ö–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è –ø–ª–∞–Ω–æ–≤ –ø—Ä–æ–µ–∫—Ç–∞ AARD")
    print("=" * 70 + "\n")
    
    consolidator = PlanConsolidator()
    
    try:
        consolidated_plan, llm_analysis = await consolidator.consolidate()
        
        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ Markdown
        output_file = consolidator.project_root / ".cursor" / "plans" / "consolidated_master_plan.md"
        output_file.parent.mkdir(parents=True, exist_ok=True)
        output_file.write_text(consolidated_plan, encoding="utf-8")
        
        # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ JSON
        json_data = consolidator.export_to_json(
            consolidator.all_tasks,
            consolidator.all_plans,
            llm_analysis
        )
        json_file = consolidator.project_root / ".cursor" / "plans" / "consolidated_master_plan.json"
        json_file.write_text(json.dumps(json_data, indent=2, ensure_ascii=False), encoding="utf-8")
        
        print(f"\n‚úÖ –ö–æ–Ω—Å–æ–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–ª–∞–Ω —Å–æ—Ö—Ä–∞–Ω–µ–Ω:")
        print(f"   Markdown: {output_file}")
        print(f"   JSON: {json_file}")
        print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"   - –ü–ª–∞–Ω–æ–≤ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {len(consolidator.all_plans)}")
        print(f"   - –ó–∞–¥–∞—á –Ω–∞–π–¥–µ–Ω–æ: {len(consolidator.all_tasks)}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å—Ç–∞—Ç—É—Å–∞–º
        completed = sum(1 for t in consolidator.all_tasks if t.get('completion', {}).get('completion_score', 0) >= 0.8)
        in_progress = sum(1 for t in consolidator.all_tasks if 0.3 <= t.get('completion', {}).get('completion_score', 0) < 0.8)
        todo = sum(1 for t in consolidator.all_tasks if t.get('completion', {}).get('completion_score', 0) < 0.3)
        
        print(f"   - –í—ã–ø–æ–ª–Ω–µ–Ω–æ: {completed}")
        print(f"   - –í –ø—Ä–æ—Ü–µ—Å—Å–µ: {in_progress}")
        print(f"   - –ù–µ –Ω–∞—á–∞—Ç–æ: {todo}")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏: {e}", exc_info=True)
        print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())

