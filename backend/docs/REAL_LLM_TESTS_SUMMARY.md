# Сводка реальных тестов с локальными LLM

## Обзор

В проекте реализовано **6 основных категорий реальных тестов** с использованием локальных LLM (Ollama на сервере 10.39.0.6):

## 1. Тесты диалогов между агентами

### `test_agent_dialogs_real_llm.py`

**Цель:** Проверка реального диалога между агентами через LLM

**Тесты:**
- `test_real_agent_dialog_with_llm` - Базовый диалог между двумя агентами
- `test_real_agent_dialog_multiturn_llm` - Многораундовый диалог

**Что тестируется:**
- Создание агентов (Planner и Developer)
- Создание диалога между агентами
- Генерация сообщений через реальные LLM вызовы
- Обмен сообщениями между агентами
- Завершение диалога

**Таймауты:**
- LLM вызов: 30 секунд
- Сообщение диалога: 20 секунд
- Полный диалог: 120 секунд (2 минуты)

**Результаты:**
- ✅ Тесты проходят успешно
- Логи сохраняются в `logs/tests/agent_dialogs_real_llm_*.log`

## 2. Тесты планирования с диалогами

### `test_planning_with_dialogs_real_llm.py`

**Цель:** Проверка интеграции диалогов в процесс планирования

**Тесты:**
- `test_planning_complex_task_with_real_dialog_llm` - Планирование сложной задачи с автоматическим диалогом

**Что тестируется:**
- Определение сложной задачи
- Автоматическое создание диалога между агентами
- Проведение диалога через реальные LLM
- Использование результатов диалога при генерации плана
- Сохранение контекста в Digital Twin

**Таймауты:**
- Общий тест: 300 секунд (5 минут)

**Результаты:**
- ✅ Тест проходит успешно
- Диалог автоматически создается для сложных задач
- Результаты диалога используются при планировании

## 3. Тесты бенчмарка моделей

### `test_model_benchmark_real.py`

**Цель:** Реальное тестирование производительности всех моделей

**Тесты:**
- `test_benchmark_all_models_planning` - Бенчмарк всех моделей на задаче планирования
- `test_benchmark_all_models_code_generation` - Бенчмарк всех моделей на генерации кода

**Что тестируется:**
- Все активные не-embedding модели на сервере 10.39.0.6
- Время ответа модели
- Качество ответа
- Количество токенов
- Сохранение результатов в БД (model.details)

**Метрики:**
- Response time (секунды)
- Quality score (0-1)
- Token count
- Success rate

**Таймауты:**
- На модель: 60 секунд
- Общий тест: 600 секунд (10 минут)

**Результаты:**
- ✅ Тесты проходят успешно
- Результаты сохраняются в БД и используются ModelSelector для выбора лучших моделей
- Некоторые модели могут таймаутить (это нормально)

## 4. Тесты полного workflow

### `test_real_llm_full_workflow.py`

**Цель:** Полный цикл от запроса до результата с реальными LLM

**Что тестируется:**
- Получение запроса от пользователя
- Планирование задачи через LLM
- Генерация плана
- Выполнение шагов плана
- Генерация кода через LLM
- Выполнение кода в sandbox
- Получение результата

**Таймауты:**
- Общий workflow: 300 секунд (5 минут)

**Результаты:**
- ✅ Тест проходит успешно
- Полный цикл от запроса до результата работает

## 5. Тесты взаимодействия модулей

### `test_real_modules_interaction.py`

**Цель:** Проверка реального взаимодействия всех модулей проекта

**Тесты:**
- `test_real_modules_interaction_full_workflow` - Полный workflow с реальными модулями

**Что тестируется:**
- PlanningService с реальными LLM
- ExecutionService с реальными LLM
- AgentTeamService
- AgentDialogService
- Интеграция всех модулей

**Таймауты:**
- Общий тест: 300 секунд (5 минут)

**Результаты:**
- ✅ Тест проходит успешно
- Все модули корректно взаимодействуют

## 6. Тесты команд агентов

### `test_agent_teams_real_llm.py`

**Цель:** Проверка работы команд агентов с реальными LLM

**Что тестируется:**
- Создание команды агентов
- Координация между агентами
- Распределение задач
- Выполнение через реальные LLM

**Результаты:**
- ✅ Тесты проходят успешно

## Статистика

### Всего реальных тестов: **~10-12 тестов**

### Категории:
1. **Диалоги агентов:** 2 теста
2. **Планирование с диалогами:** 1 тест
3. **Бенчмарк моделей:** 2 теста
4. **Полный workflow:** 1 тест
5. **Взаимодействие модулей:** 1 тест
6. **Команды агентов:** 2-3 теста

### Используемый сервер:
- **Ollama:** `http://10.39.0.6:11434`
- **Модели:** Все активные не-embedding модели

### Особенности:
- Все тесты используют реальные LLM вызовы
- Таймауты настроены для ограниченного железа
- Логирование в отдельные файлы
- Результаты сохраняются в БД
- Тесты помечены маркером `@pytest.mark.slow`

## Запуск тестов

```bash
# Все реальные тесты
pytest backend/tests/integration/ -m slow -v

# Конкретный тест
pytest backend/tests/integration/test_agent_dialogs_real_llm.py::test_real_agent_dialog_with_llm -v

# Бенчмарк моделей
pytest backend/tests/integration/test_model_benchmark_real.py -v
```

## Логи

Все логи реальных тестов сохраняются в:
- `logs/tests/agent_dialogs_real_llm_*.log`
- `logs/tests/planning_with_dialogs_real_llm_*.log`
- `logs/tests/model_benchmark_*.log`

