"""
Integration tests for Phase 6 (A/B Testing) consistency with other modules
"""
import pytest
import asyncio
from uuid import uuid4

from app.core.database import SessionLocal
from app.models.task import Task, TaskStatus
from app.models.plan import Plan, PlanStatus
from app.services.planning_service import PlanningService
from app.services.execution_service import ExecutionService
from app.services.decision_pipeline import DecisionPipeline
from app.services.plan_evaluation_service import PlanEvaluationService


@pytest.fixture
def db():
    """Database session fixture"""
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()


@pytest.fixture
def planning_service(db):
    """PlanningService fixture"""
    return PlanningService(db)


@pytest.fixture
def execution_service(db):
    """ExecutionService fixture"""
    return ExecutionService(db)


@pytest.fixture
def decision_pipeline(db):
    """DecisionPipeline fixture"""
    return DecisionPipeline(db)


@pytest.fixture
def test_task(db):
    """Create a test task"""
    task = Task(
        id=uuid4(),
        description="Test task for consistency testing",
        status=TaskStatus.PENDING,
        priority=5
    )
    db.add(task)
    db.commit()
    db.refresh(task)
    return task


@pytest.mark.asyncio
async def test_api_endpoint_backward_compatibility(planning_service, test_task):
    """Test that API endpoint calls work without new parameters (backward compatibility)"""
    task_description = "Create a simple REST API"
    
    # Call generate_plan without new parameters (simulating old API calls)
    plan = await planning_service.generate_plan(
        task_description=task_description,
        task_id=test_task.id
    )
    
    # Should work without errors
    assert plan is not None
    assert plan.task_id == test_task.id
    assert plan.goal is not None


@pytest.mark.asyncio
async def test_api_endpoint_with_new_parameters(planning_service, test_task):
    """Test that API endpoint calls work with new parameters"""
    task_description = "Create a fast API endpoint"
    
    # Call generate_plan with new parameters
    plan = await planning_service.generate_plan(
        task_description=task_description,
        task_id=test_task.id,
        generate_alternatives=True,
        num_alternatives=2
    )
    
    # Should work with new parameters
    assert plan is not None
    assert plan.task_id == test_task.id


@pytest.mark.asyncio
async def test_execution_service_integration(planning_service, execution_service, test_task):
    """Test that ExecutionService works with plans generated by PlanningService"""
    task_description = "Execute a simple task"
    
    # Generate plan (without alternatives for simplicity)
    plan = await planning_service.generate_plan(
        task_description=task_description,
        task_id=test_task.id,
        generate_alternatives=False
    )
    
    # Approve plan
    plan.status = PlanStatus.APPROVED.value
    planning_service.db.commit()
    
    # ExecutionService should be able to work with this plan
    # (We don't actually execute, just verify the plan is compatible)
    assert plan.id is not None
    assert plan.status == PlanStatus.APPROVED.value
    assert plan.steps is not None


@pytest.mark.asyncio
async def test_decision_pipeline_integration(decision_pipeline, test_task):
    """Test that DecisionPipeline works with PlanningService changes"""
    task_description = "Execute task through decision pipeline"
    
    # DecisionPipeline uses PlanningService internally
    # It should work with default parameters
    result = await decision_pipeline.execute_task(
        task_description=task_description,
        context={"task_id": str(test_task.id)}
    )
    
    # Should complete without errors (may fail at execution, but planning should work)
    assert result is not None
    assert "status" in result
    assert "pipeline_metadata" in result


@pytest.mark.asyncio
async def test_replan_backward_compatibility(planning_service, test_task):
    """Test that replan method works with new generate_plan signature"""
    task_description = "Initial task description"
    
    # Create initial plan
    plan = await planning_service.generate_plan(
        task_description=task_description,
        task_id=test_task.id
    )
    
    # Approve plan
    plan.status = PlanStatus.APPROVED.value
    planning_service.db.commit()
    
    # Replan should work (it calls generate_plan internally)
    # Note: replan doesn't take new_task_description, it uses task.description
    new_plan = await planning_service.replan(
        plan_id=plan.id,
        reason="Test replanning",
        context={"updated_requirements": "New requirements"}
    )
    
    # Should create new plan
    assert new_plan is not None
    assert new_plan.id != plan.id


@pytest.mark.asyncio
async def test_plan_evaluation_service_integration(planning_service, test_task):
    """Test that PlanEvaluationService works with plans from PlanningService"""
    task_description = "Create a web application"
    
    # Generate plan with alternatives
    best_plan = await planning_service.generate_plan(
        task_description=task_description,
        task_id=test_task.id,
        generate_alternatives=True,
        num_alternatives=2
    )
    
    # PlanEvaluationService should be able to evaluate this plan
    evaluation_service = PlanEvaluationService(planning_service.db)
    result = evaluation_service.evaluate_plan(best_plan)
    
    # Should return evaluation result
    assert result is not None
    assert result.total_score is not None
    assert result.scores is not None
    assert len(result.scores) > 0


@pytest.mark.asyncio
async def test_plan_metadata_consistency(planning_service, test_task):
    """Test that plan metadata is consistent across different generation modes"""
    task_description = "Test metadata consistency"
    
    # Generate plan without alternatives
    plan1 = await planning_service.generate_plan(
        task_description=task_description,
        task_id=test_task.id,
        generate_alternatives=False
    )
    
    # Generate plan with alternatives
    plan2 = await planning_service.generate_plan(
        task_description=task_description,
        task_id=test_task.id,
        generate_alternatives=True,
        num_alternatives=2
    )
    
    # Both plans should have consistent structure
    assert plan1.id is not None
    assert plan2.id is not None
    assert plan1.goal is not None
    assert plan2.goal is not None
    assert plan1.strategy is not None or plan1.strategy == {}
    assert plan2.strategy is not None or plan2.strategy == {}
    assert plan1.steps is not None
    assert plan2.steps is not None


@pytest.mark.asyncio
async def test_context_passing_consistency(planning_service, test_task):
    """Test that context is passed correctly in all modes"""
    task_description = "Test context passing"
    context = {
        "test_key": "test_value",
        "constraints": ["Must use PostgreSQL"]
    }
    
    # Test without alternatives
    plan1 = await planning_service.generate_plan(
        task_description=task_description,
        task_id=test_task.id,
        context=context,
        generate_alternatives=False
    )
    
    # Test with alternatives
    plan2 = await planning_service.generate_plan(
        task_description=task_description,
        task_id=test_task.id,
        context=context,
        generate_alternatives=True,
        num_alternatives=2
    )
    
    # Both should work
    assert plan1 is not None
    assert plan2 is not None


@pytest.mark.asyncio
async def test_custom_evaluation_weights_consistency(planning_service, test_task):
    """Test that custom evaluation weights work correctly"""
    task_description = "Test custom weights"
    weights = {
        "execution_time": 0.5,
        "approval_points": 0.1,
        "risk_level": 0.2,
        "efficiency": 0.2
    }
    
    # Generate plan with custom weights
    plan = await planning_service.generate_plan(
        task_description=task_description,
        task_id=test_task.id,
        generate_alternatives=True,
        num_alternatives=2,
        evaluation_weights=weights
    )
    
    # Should work without errors
    assert plan is not None
    
    # Check that best plan has evaluation metadata
    if plan.alternatives and isinstance(plan.alternatives, dict):
        assert "evaluation_score" in plan.alternatives or "is_best" in plan.alternatives


@pytest.mark.asyncio
async def test_alternative_plans_method_consistency(planning_service, test_task):
    """Test that generate_alternative_plans method still works independently"""
    task_description = "Test alternative plans method"
    
    # Call generate_alternative_plans directly
    alternative_plans = await planning_service.generate_alternative_plans(
        task_description=task_description,
        task_id=test_task.id,
        num_alternatives=2
    )
    
    # Should return list of plans
    assert alternative_plans is not None
    assert isinstance(alternative_plans, list)
    assert len(alternative_plans) >= 1
    
    # Each plan should have alternative metadata
    for plan in alternative_plans:
        assert plan.id is not None
        if plan.strategy and isinstance(plan.strategy, dict):
            # Should have alternative_strategy if metadata was saved
            pass  # Metadata may or may not be present depending on save success


if __name__ == "__main__":
    pytest.main([__file__, "-v"])

