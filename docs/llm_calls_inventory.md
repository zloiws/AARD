# LLM Calls Inventory

Generated by `tools/collect_llm_calls.py`.

| file | line | container | classification | task_type | has_system_prompt | prompt_source | call |
| --- | --- | --- | --- | --- | --- | --- | --- |
| backend\app\agents\base_agent.py | 178 | BaseAgent._call_llm | agent | Name(id='task_type', ctx=Load()) | yes | inline | self.ollama_client.generate( prompt=prompt, system_prompt=system_prompt, task_type=task_type, model=actual_model, temperature=temperature, **kwargs ) |
| backend\app\core\decision_framework.py | 279 | NaturalLanguageReasoner.reason | infra | Constant(value=None) | no | inline | self.ollama_client.generate( prompt=full_prompt, task_type=None, temperature=0.7 ) |
| backend\app\core\decision_framework.py | 312 | NaturalLanguageReasoner.understand | infra | Constant(value=None) | no | inline | self.ollama_client.generate( prompt=prompt, task_type=None, temperature=0.3 ) |
| backend\app\core\decision_framework.py | 360 | NaturalLanguageReasoner.generate_ideas | infra | Constant(value=None) | no | inline | self.ollama_client.generate( prompt=prompt, task_type=None, temperature=0.8 ) |
| backend\app\core\ollama_db_client.py | 58 | OllamaDBClient.generate | infra | unknown | no | unknown | self.client.generate(*args, **kwargs) |
| backend\app\core\request_orchestrator.py | 354 | RequestOrchestrator._handle_simple_question | infra | IfExp(test=UnaryOp(op=Not(), operand=Name(id='task_type', ctx=Load())), body=Attribute(value=Name(id='TaskType', ctx=Load()), attr='DEFAULT', ctx=Load()), orelse=Call(func=Name(id='TaskType', ctx=Load()), args=[Name(id='task_type', ctx=Load())], keywords=[])) | yes | inline | ollama_client.generate( prompt=message, task_type=TaskType.DEFAULT if not task_type else TaskType(task_type), model=selected_model, server_url=selected_serve... |
| backend\app\services\agent_approval_agent.py | 281 | AgentApprovalAgent._assess_agent_necessity | unknown | unknown | yes | inline | self.ollama_client.generate( prompt=assessment_prompt, system_prompt="Ты эксперт по анализу архитектуры агентных систем. Оценивай необходимость создания новы... |
| backend\app\services\agent_dialog_service.py | 438 | AgentDialogService.complete_conversation | unknown | unknown | no | unknown | conversation.complete(success=success) |
| backend\app\services\agent_evolution_service.py | 228 | AgentEvolutionService._generate_improvement_suggestions | unknown | reasoning | no | inline | self.ollama_client.generate( prompt=prompt, task_type="reasoning", model=None, # Use default reasoning model temperature=0.7 ) |
| backend\app\services\agent_evolution_service.py | 373 | AgentEvolutionService._apply_suggestions_to_prompt | unknown | reasoning | no | inline | self.ollama_client.generate( prompt=improvement_prompt, task_type="reasoning", model=None, temperature=0.7 ) |
| backend\app\services\artifact_generator.py | 171 | ArtifactGenerator._analyze_requirements | unknown | TaskType.REASONING | yes | inline | self.ollama_client.generate( prompt=user_prompt, system_prompt=system_prompt, task_type=TaskType.REASONING ) |
| backend\app\services\artifact_generator.py | 254 | ArtifactGenerator._generate_tool_code | unknown | TaskType.CODE_GENERATION | yes | inline | self.ollama_client.generate( prompt=user_prompt, system_prompt=system_prompt, task_type=TaskType.CODE_GENERATION ) |
| backend\app\services\artifact_generator.py | 301 | ArtifactGenerator._generate_agent_prompt | unknown | TaskType.REASONING | yes | inline | self.ollama_client.generate( prompt=user_prompt, system_prompt=system_prompt, task_type=TaskType.REASONING ) |
| backend\app\services\benchmark_service.py | 242 | BenchmarkService.run_benchmark | unknown | Name(id='ollama_task_type', ctx=Load()) | yes | inline | client.generate( prompt=user_prompt, task_type=ollama_task_type, model=model_name, server_url=server_url, system_prompt=system_prompt, temperature=0.7, # Exp... |
| backend\app\services\benchmark_service.py | 526 | BenchmarkService._llm_evaluate | unknown | TaskType.REASONING | no | inline | client.generate( prompt=eval_prompt, task_type=TaskType.REASONING ) |
| backend\app\services\critic_service.py | 300 | CriticService._llm_semantic_check | unknown | Constant(value=None) | no | inline | self.ollama_client.generate( prompt=prompt, task_type=None, temperature=0.3 ) |
| backend\app\services\execution_service.py | 591 | StepExecutor._execute_action_step | unknown | code_generation | yes | inline | ollama_client.generate( prompt=user_prompt, server_url=server.get_api_url(), model=execution_model.model_name or execution_model.name, system_prompt=system_p... |
| backend\app\services\execution_service.py | 1043 | StepExecutor._execute_decision_step | unknown | unknown | no | inline | ollama_client.generate( model=reasoning_model, prompt=decision_prompt, temperature=0.3, # Lower temperature for more deterministic decisions format="json" ) |
| backend\app\services\model_benchmark_service.py | 84 | ModelBenchmarkService.benchmark_model | unknown | Name(id='task_type', ctx=Load()) | no | inline | self.ollama_client.generate( prompt=prompt, task_type=task_type, model=model.model_name, server_url=server_url, num_predict=200 # Ограничение для быстрого те... |
| backend\app\services\planning_service.py | 2330 | PlanningService._analyze_task | unknown | TaskType.PLANNING | yes | inline | ollama_client.generate( prompt=user_prompt, system_prompt=system_prompt, task_type=TaskType.PLANNING, model=planning_model.model_name, server_url=server.get_... |
| backend\app\services\planning_service.py | 2710 | PlanningService._decompose_task | unknown | TaskType.PLANNING | yes | inline | ollama_client.generate( prompt=user_prompt, system_prompt=system_prompt, task_type=TaskType.PLANNING, model=planning_model.model_name, server_url=server.get_... |
| backend\app\services\planning_service.py | 4096 | PlanningService._adapt_template_to_task | unknown | TaskType.PLANNING | yes | inline | ollama_client.generate( prompt=user_prompt, system_prompt=system_prompt, task_type=TaskType.PLANNING, model=planning_model.model_name, server_url=server.get_... |
| backend\app\services\planning_service_dialog_integration.py | 235 | conduct_agent_dialog | unknown | TaskType.PLANNING | no | inline | ollama_client.generate( prompt=agent1_prompt, task_type=TaskType.PLANNING, model=planning_model.model_name, server_url=server_url, num_predict=200, use_cache... |
| backend\app\services\planning_service_dialog_integration.py | 269 | conduct_agent_dialog | unknown | TaskType.PLANNING | no | inline | ollama_client.generate( prompt=agent2_prompt, task_type=TaskType.PLANNING, model=planning_model.model_name, server_url=server_url, num_predict=200, use_cache... |
| backend\app\services\prompt_service.py | 908 | PromptService._generate_llm_suggestions | unknown | TaskType.PLANNING | yes | inline | ollama_client.generate( prompt=user_prompt, system_prompt=system_prompt, task_type=TaskType.PLANNING, model=planning_model.model_name, server_url=server.get_... |
| backend\app\services\prompt_service.py | 1081 | PromptService._generate_improved_prompt_text | unknown | TaskType.PLANNING | yes | inline | ollama_client.generate( prompt=user_prompt, system_prompt=system_prompt, task_type=TaskType.PLANNING, model=planning_model.model_name, server_url=server.get_... |
| backend\app\services\reflection_service.py | 271 | ReflectionService._llm_analyze_failure | unknown | Constant(value=None) | yes | inline | self.ollama_client.generate( prompt=user_prompt, system_prompt=system_prompt, task_type=None, temperature=0.3 ) |
| backend\app\services\reflection_service.py | 423 | ReflectionService._llm_generate_fix | unknown | Constant(value=None) | yes | inline | self.ollama_client.generate( prompt=user_prompt, system_prompt=system_prompt, task_type=None, temperature=0.5 ) |
| backend\app\services\self_audit_service.py | 682 | SelfAuditService._generate_llm_summary | unknown | reasoning | no | inline | client.generate( prompt=prompt, task_type="reasoning" ) |
| backend\app\services\self_audit_service.py | 1075 | SelfAuditService.analyze_trends_with_llm | unknown | reasoning | no | inline | client.generate( prompt=prompt, task_type="reasoning" ) |
| backend\app\services\uncertainty_learning_service.py | 371 | UncertaintyLearningService.update_keyword_lists_with_llm | unknown | TaskType.REASONING | yes | inline | self.ollama_client.generate( prompt=prompt, system_prompt="Ты эксперт по анализу текста и обнаружению неопределенности. Предлагай точные и эффективные ключев... |
