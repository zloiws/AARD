# Незавершенные функции и задачи

## Исправлено ✅

### 1. ExecutionService - StepExecutor.metrics_service
**Проблема:** `StepExecutor` не имел `metrics_service`, но использовался в `execute_step`
**Исправлено:** Добавлен параметр `metrics_service` в конструктор `StepExecutor` и передача из `ExecutionService`

### 2. BaseAgent - tool_service в конструкторе
**Проблема:** `SimpleAgent` создавался с `tool_service=tool_service`, но `BaseAgent.__init__` не принимает этот параметр
**Исправлено:** Убрана передача `tool_service`, используется `db_session` для создания внутри `BaseAgent`

## Требует доработки

### 1. Управление моделями и GPU (docs/archive/TODO_MODEL_MANAGEMENT.md)

#### 1.1 Правильное чтение состояния модели
- [ ] Улучшить обработку ответа от `/api/ps` Ollama API
- [ ] Добавить кэширование состояния модели
- [ ] Добавить периодическую проверку состояния загруженных моделей
- [ ] Отображать статус модели в UI (загружена/не загружена)

#### 1.2 Правильная выгрузка модели из GPU
- [ ] Исследовать возможности Ollama API для принудительной выгрузки
- [ ] Реализовать workaround (если доступен)
- [ ] Если прямого метода нет - реализовать ожидание таймаута
- [ ] Добавить логирование операций выгрузки

#### 1.3 Динамическая замена модели при переключении
- [ ] Интегрировать проверку загруженной модели в `OllamaClient.generate()`
- [ ] Добавить логику выгрузки перед загрузкой новой модели
- [ ] Обработать ошибки (если выгрузка не удалась)
- [ ] Добавить настройку: автоматическая замена или ручная
- [ ] Добавить логирование всех операций

#### 1.4 Мониторинг использования GPU
- [ ] Добавить endpoint для получения списка загруженных моделей
- [ ] Отображать использование GPU памяти
- [ ] Показывать время загрузки модели
- [ ] Предупреждать о нехватке памяти

#### 1.5 Управление приоритетами моделей
- [ ] Автоматическая выгрузка моделей с низким приоритетом
- [ ] Сохранение "горячих" моделей в GPU
- [ ] Настройка таймаутов для разных моделей

### 2. Frontend - незавершенные разделы

#### 2.1 Мульти-модель чат
- [ ] Реализовать функционал чата между моделями
- [ ] Настройка параметров для каждой модели
- [ ] Отображение диалога между моделями

#### 2.2 Метрики и статистика
- [ ] Реализовать раздел метрик
- [ ] Визуализация данных
- [ ] Экспорт метрик

### 3. Streaming responses через WebSocket
- [ ] Реализовать streaming через WebSocket (docs/OLLAMA_INTEGRATION.md)
- [ ] Интеграция с chat API
- [ ] Обработка ошибок при streaming

### 4. Plan Template Service
- [ ] Реализовать async LLM abstraction (TODO в plan_template_service.py:313)
- [ ] Улучшить pattern matching для шаблонов

### 5. Тестирование
- [ ] Увеличить покрытие тестами
- [ ] Добавить тесты для всех сервисов
- [ ] Добавить E2E тесты для всех workflow

## Приоритеты

### Высокий приоритет
1. ✅ Исправление ошибок в ExecutionService и BaseAgent (выполнено)
2. Управление моделями и GPU (критично для работы с ограниченными ресурсами)
3. Streaming responses (улучшение UX)

### Средний приоритет
4. Frontend - незавершенные разделы
5. Plan Template Service - async LLM

### Низкий приоритет
6. Увеличение покрытия тестами
7. Дополнительные улучшения

